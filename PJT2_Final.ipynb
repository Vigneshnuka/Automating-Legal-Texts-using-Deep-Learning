{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Connecting Drive to Colab for files access\n"
      ],
      "metadata": {
        "id": "_DEzzLEj_wPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMFRtCQfdtn0",
        "outputId": "a6342248-3f08-4b03-f691-8194ddde4807"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Libraries Installation"
      ],
      "metadata": {
        "id": "0RJFHXT-_65o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydub\n",
        "!pip install PyPDF2==3.0.1\n",
        "!pip install pdfplumber\n",
        "!pip install transformers torch pdfplumber\n",
        "!pip install textblob==0.19.0\n",
        "!python -m textblob.download_corpora"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KHzA-vyeGO7",
        "outputId": "fc80347a-aa81-4250-c153-5025c7ab31a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2==3.0.1\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m204.8/232.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.5-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20231228 (from pdfplumber)\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.1.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Downloading pdfplumber-0.11.5-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20231228 pdfplumber-0.11.5 pypdfium2-4.30.1\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.11/dist-packages (0.11.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pdfminer.six==20231228 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (20231228)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.1.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (4.30.1)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: textblob==0.19.0 in /usr/local/lib/python3.11/dist-packages (0.19.0)\n",
            "Requirement already satisfied: nltk>=3.9 in /usr/local/lib/python3.11/dist-packages (from textblob==0.19.0) (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob==0.19.0) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob==0.19.0) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob==0.19.0) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob==0.19.0) (4.67.1)\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "Finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prprocessinf of Text and\n",
        "Text Extraction using BERT\n",
        "\n",
        "\n",
        "1.   Extraction using BERT Question Answering (If extraction is less than 400 goto 2)\n",
        "2.   Rule based Extraction (If combination of 1 and 2 is less than 400 goto 3)\n",
        "3.   Extraction from PDF document related to key allegations and all\n",
        "\n"
      ],
      "metadata": {
        "id": "sVSbJJXV_-Jf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import torch\n",
        "import pdfplumber\n",
        "from transformers import BertTokenizer, BertForQuestionAnswering\n",
        "\n",
        "# Load BERT model & tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
        "model = BertForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
        "\n",
        "# Extract text from PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        text = \" \".join(page.extract_text() for page in pdf.pages if page.extract_text())\n",
        "    return text\n",
        "\n",
        "# Remove URLs & unnecessary text\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)  # Remove URLs\n",
        "    text = re.sub(r'Page\\s*\\d+', '', text)  # Remove page numbers\n",
        "    text = re.sub(r'\\s+', ' ', text.strip())  # Remove excessive spaces\n",
        "    return text\n",
        "\n",
        "# Sliding Window Tokenization for BERT (Handles Long Documents)\n",
        "def split_text_sliding_window(text, max_length=512, overlap=50):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    i = 0\n",
        "    while i < len(words):\n",
        "        chunk = \" \".join(words[i:i + max_length])\n",
        "        chunks.append(chunk)\n",
        "        i += max_length - overlap  # Sliding window (overlapping text)\n",
        "    return chunks\n",
        "\n",
        "# Extract judgment text using BERT\n",
        "def extract_judgment_with_bert(context):\n",
        "    chunks = split_text_sliding_window(context)\n",
        "    answers = []\n",
        "\n",
        "    for chunk in chunks:\n",
        "        inputs = tokenizer.encode_plus(\"final judgment\", chunk, add_special_tokens=True, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "        input_ids = inputs[\"input_ids\"]\n",
        "        outputs = model(**inputs)\n",
        "        start_index = torch.argmax(outputs.start_logits)\n",
        "        end_index = torch.argmax(outputs.end_logits)\n",
        "        answer_tokens = input_ids[0][start_index:end_index + 1]\n",
        "        answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n",
        "        answers.append(answer)\n",
        "\n",
        "    # Merge extracted chunks & check word count\n",
        "    extracted_text = \" \".join(answers).strip()\n",
        "    return extracted_text\n",
        "\n",
        "# Rule-based fallback if BERT fails (prioritize summary or key arguments)\n",
        "def rule_based_fallback(text, word_limit=900):\n",
        "    # Keywords for identifying summary or key arguments\n",
        "    summary_keywords = [\n",
        "        \"summary\", \"conclusion\", \"key argument\", \"final decision\", \"decision\",\n",
        "        \"court's findings\", \"main points\", \"court ruled\", \"reasoning\", \"holding\"\n",
        "    ]\n",
        "\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "\n",
        "    # Extract sentences that contain any of the summary or key argument keywords\n",
        "    extracted_sentences = [s for s in sentences if any(keyword in s.lower() for keyword in summary_keywords)]\n",
        "\n",
        "    if not extracted_sentences:\n",
        "        print(\"Fallback: No key sections found. Extracting first 900 words as fallback.\")\n",
        "        extracted_sentences = sentences[:word_limit]\n",
        "\n",
        "    extracted_text = \" \".join(extracted_sentences)\n",
        "    return extracted_text\n",
        "\n",
        "# Extract fallback content if both BERT & rule-based extraction fail\n",
        "def fallback_to_text_length_based(text, min_words=500, max_words=900):\n",
        "    words = text.split()\n",
        "    if len(words) >= min_words and len(words) <= max_words:\n",
        "        return text  # If the text is within the required word count\n",
        "    else:\n",
        "        print(f\"Fallback: Extracting a portion of the document between {min_words} and {max_words} words.\")\n",
        "        return \" \".join(words[:max_words]) if len(words) > max_words else text\n",
        "\n",
        "# Ensure the final text is between 500 and 900 words\n",
        "def ensure_word_count(text, min_words=500, max_words=900):\n",
        "    words = text.split()\n",
        "    if len(words) < min_words:\n",
        "        print(f\"Final Fallback: Text is less than {min_words} words, increasing content.\")\n",
        "        return \" \".join(words[:max_words])  # Take up to max_words if text is too short\n",
        "    elif len(words) > max_words:\n",
        "        return \" \".join(words[:max_words])  # Trim if it's too long\n",
        "    return text\n",
        "\n",
        "# Process PDFs & save extracted judgments\n",
        "def process_pdfs_in_folder(input_folder, output_folder, file_list):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    for pdf_file in file_list:\n",
        "        pdf_path = os.path.join(input_folder, pdf_file)\n",
        "        print(f\"Processing file: {pdf_file}\")\n",
        "\n",
        "        raw_text = extract_text_from_pdf(pdf_path)\n",
        "        if not raw_text.strip():\n",
        "            print(f\"Skipping {pdf_file} as no extractable text was found.\")\n",
        "            continue\n",
        "\n",
        "        cleaned_text = clean_text(raw_text)\n",
        "\n",
        "        # First extraction: BERT model\n",
        "        judgment_text_bert = extract_judgment_with_bert(cleaned_text)\n",
        "\n",
        "        # Check if BERT extraction is less than 500 words\n",
        "        words_bert = judgment_text_bert.split()\n",
        "        if len(words_bert) < 500:\n",
        "            print(f\"First extraction (BERT) is less than 500 words. Applying rule-based fallback.\")\n",
        "            # Second extraction: Rule-based fallback\n",
        "            judgment_text_rule_based = rule_based_fallback(cleaned_text)\n",
        "            combined_text = judgment_text_bert + \" \" + judgment_text_rule_based\n",
        "        else:\n",
        "            combined_text = judgment_text_bert\n",
        "\n",
        "        # If combined text is still less than 500 words, fallback to length-based extraction\n",
        "        words_combined = combined_text.split()\n",
        "        if len(words_combined) < 500:\n",
        "            print(f\"Combined text is still less than 500 words. Applying length-based extraction.\")\n",
        "            judgment_text_length_based = fallback_to_text_length_based(cleaned_text)\n",
        "            combined_text = combined_text + \" \" + judgment_text_length_based\n",
        "\n",
        "        # Ensure the final text is between 500 and 900 words\n",
        "        final_text = ensure_word_count(combined_text)\n",
        "\n",
        "        # Save the extracted text\n",
        "        output_file = os.path.join(output_folder, f\"{os.path.splitext(pdf_file)[0]}.txt\")\n",
        "        with open(output_file, \"w\") as file:\n",
        "            file.write(final_text)\n",
        "\n",
        "        print(f\"Processed & saved: {output_file}\")\n",
        "\n",
        "# Get all PDF files in the folder (no limit)\n",
        "def get_all_files(input_folder):\n",
        "    pdf_files = [f for f in os.listdir(input_folder) if f.lower().endswith('.pdf')]\n",
        "    return pdf_files\n",
        "\n",
        "# Paths\n",
        "input_folder = \"/content/drive/MyDrive/Dataset/Case_Files/PDFs\"\n",
        "output_folder = \"/content/extracted_text\"\n",
        "\n",
        "# Get all PDF files\n",
        "all_files = get_all_files(input_folder)\n",
        "\n",
        "# Process PDFs and save to extracted_text folder\n",
        "process_pdfs_in_folder(input_folder, output_folder, all_files)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaxCBnSciOlL",
        "outputId": "25844264-07b6-46c5-97f5-28e97fdc5782"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file: Case_1.pdf\n",
            "First extraction (BERT) is less than 500 words. Applying rule-based fallback.\n",
            "Fallback: No key sections found. Extracting first 900 words as fallback.\n",
            "Combined text is still less than 500 words. Applying length-based extraction.\n",
            "Fallback: Extracting a portion of the document between 500 and 900 words.\n",
            "Processed & saved: /content/extracted_text/Case_1.txt\n",
            "Processing file: Case_2.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed & saved: /content/extracted_text/Case_2.txt\n",
            "Processing file: Case_3.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed & saved: /content/extracted_text/Case_3.txt\n",
            "Processing file: Case_4.pdf\n",
            "First extraction (BERT) is less than 500 words. Applying rule-based fallback.\n",
            "Fallback: No key sections found. Extracting first 900 words as fallback.\n",
            "Combined text is still less than 500 words. Applying length-based extraction.\n",
            "Fallback: Extracting a portion of the document between 500 and 900 words.\n",
            "Processed & saved: /content/extracted_text/Case_4.txt\n",
            "Processing file: Case_5.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First extraction (BERT) is less than 500 words. Applying rule-based fallback.\n",
            "Combined text is still less than 500 words. Applying length-based extraction.\n",
            "Fallback: Extracting a portion of the document between 500 and 900 words.\n",
            "Processed & saved: /content/extracted_text/Case_5.txt\n",
            "Processing file: Case_6.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First extraction (BERT) is less than 500 words. Applying rule-based fallback.\n",
            "Fallback: No key sections found. Extracting first 900 words as fallback.\n",
            "Processed & saved: /content/extracted_text/Case_6.txt\n",
            "Processing file: Case_7.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First extraction (BERT) is less than 500 words. Applying rule-based fallback.\n",
            "Fallback: No key sections found. Extracting first 900 words as fallback.\n",
            "Processed & saved: /content/extracted_text/Case_7.txt\n",
            "Processing file: Case_8.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First extraction (BERT) is less than 500 words. Applying rule-based fallback.\n",
            "Fallback: No key sections found. Extracting first 900 words as fallback.\n",
            "Processed & saved: /content/extracted_text/Case_8.txt\n",
            "Processing file: Case_9.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First extraction (BERT) is less than 500 words. Applying rule-based fallback.\n",
            "Fallback: No key sections found. Extracting first 900 words as fallback.\n",
            "Processed & saved: /content/extracted_text/Case_9.txt\n",
            "Processing file: Case_10.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First extraction (BERT) is less than 500 words. Applying rule-based fallback.\n",
            "Combined text is still less than 500 words. Applying length-based extraction.\n",
            "Fallback: Extracting a portion of the document between 500 and 900 words.\n",
            "Processed & saved: /content/extracted_text/Case_10.txt\n",
            "Processing file: Case_11.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First extraction (BERT) is less than 500 words. Applying rule-based fallback.\n",
            "Processed & saved: /content/extracted_text/Case_11.txt\n",
            "Processing file: Case_12.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First extraction (BERT) is less than 500 words. Applying rule-based fallback.\n",
            "Fallback: No key sections found. Extracting first 900 words as fallback.\n",
            "Processed & saved: /content/extracted_text/Case_12.txt\n",
            "Processing file: Case_13.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First extraction (BERT) is less than 500 words. Applying rule-based fallback.\n",
            "Fallback: No key sections found. Extracting first 900 words as fallback.\n",
            "Processed & saved: /content/extracted_text/Case_13.txt\n",
            "Processing file: Case_14.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First extraction (BERT) is less than 500 words. Applying rule-based fallback.\n",
            "Fallback: No key sections found. Extracting first 900 words as fallback.\n",
            "Processed & saved: /content/extracted_text/Case_14.txt\n",
            "Processing file: Case_15.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First extraction (BERT) is less than 500 words. Applying rule-based fallback.\n",
            "Fallback: No key sections found. Extracting first 900 words as fallback.\n",
            "Combined text is still less than 500 words. Applying length-based extraction.\n",
            "Fallback: Extracting a portion of the document between 500 and 900 words.\n",
            "Final Fallback: Text is less than 500 words, increasing content.\n",
            "Processed & saved: /content/extracted_text/Case_15.txt\n",
            "Processing file: Case_16.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First extraction (BERT) is less than 500 words. Applying rule-based fallback.\n",
            "Fallback: No key sections found. Extracting first 900 words as fallback.\n",
            "Processed & saved: /content/extracted_text/Case_16.txt\n",
            "Processing file: Case_17.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed & saved: /content/extracted_text/Case_17.txt\n",
            "Processing file: Case_18.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed & saved: /content/extracted_text/Case_18.txt\n",
            "Processing file: Case_19.pdf\n",
            "First extraction (BERT) is less than 500 words. Applying rule-based fallback.\n",
            "Fallback: No key sections found. Extracting first 900 words as fallback.\n",
            "Combined text is still less than 500 words. Applying length-based extraction.\n",
            "Fallback: Extracting a portion of the document between 500 and 900 words.\n",
            "Final Fallback: Text is less than 500 words, increasing content.\n",
            "Processed & saved: /content/extracted_text/Case_19.txt\n",
            "Processing file: Case_20.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed & saved: /content/extracted_text/Case_20.txt\n",
            "Processing file: Case_21.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First extraction (BERT) is less than 500 words. Applying rule-based fallback.\n",
            "Fallback: No key sections found. Extracting first 900 words as fallback.\n",
            "Combined text is still less than 500 words. Applying length-based extraction.\n",
            "Fallback: Extracting a portion of the document between 500 and 900 words.\n",
            "Processed & saved: /content/extracted_text/Case_21.txt\n",
            "Processing file: Case_22.pdf\n",
            "First extraction (BERT) is less than 500 words. Applying rule-based fallback.\n",
            "Fallback: No key sections found. Extracting first 900 words as fallback.\n",
            "Processed & saved: /content/extracted_text/Case_22.txt\n",
            "Processing file: Case_23.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First extraction (BERT) is less than 500 words. Applying rule-based fallback.\n",
            "Combined text is still less than 500 words. Applying length-based extraction.\n",
            "Fallback: Extracting a portion of the document between 500 and 900 words.\n",
            "Processed & saved: /content/extracted_text/Case_23.txt\n",
            "Processing file: Case_24.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First extraction (BERT) is less than 500 words. Applying rule-based fallback.\n",
            "Combined text is still less than 500 words. Applying length-based extraction.\n",
            "Fallback: Extracting a portion of the document between 500 and 900 words.\n",
            "Processed & saved: /content/extracted_text/Case_24.txt\n",
            "Processing file: Case_25.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First extraction (BERT) is less than 500 words. Applying rule-based fallback.\n",
            "Processed & saved: /content/extracted_text/Case_25.txt\n",
            "Processing file: Case_26.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First extraction (BERT) is less than 500 words. Applying rule-based fallback.\n",
            "Combined text is still less than 500 words. Applying length-based extraction.\n",
            "Fallback: Extracting a portion of the document between 500 and 900 words.\n",
            "Processed & saved: /content/extracted_text/Case_26.txt\n",
            "Processing file: Case_27.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed & saved: /content/extracted_text/Case_27.txt\n",
            "Processing file: Case_28.pdf\n",
            "First extraction (BERT) is less than 500 words. Applying rule-based fallback.\n",
            "Fallback: No key sections found. Extracting first 900 words as fallback.\n",
            "Processed & saved: /content/extracted_text/Case_28.txt\n",
            "Processing file: Case_29.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First extraction (BERT) is less than 500 words. Applying rule-based fallback.\n",
            "Combined text is still less than 500 words. Applying length-based extraction.\n",
            "Fallback: Extracting a portion of the document between 500 and 900 words.\n",
            "Processed & saved: /content/extracted_text/Case_29.txt\n",
            "Processing file: Case_30.pdf\n",
            "First extraction (BERT) is less than 500 words. Applying rule-based fallback.\n",
            "Fallback: No key sections found. Extracting first 900 words as fallback.\n",
            "Combined text is still less than 500 words. Applying length-based extraction.\n",
            "Fallback: Extracting a portion of the document between 500 and 900 words.\n",
            "Final Fallback: Text is less than 500 words, increasing content.\n",
            "Processed & saved: /content/extracted_text/Case_30.txt\n",
            "Processing file: Case_31.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed & saved: /content/extracted_text/Case_31.txt\n",
            "Processing file: Case_32.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First extraction (BERT) is less than 500 words. Applying rule-based fallback.\n",
            "Combined text is still less than 500 words. Applying length-based extraction.\n",
            "Fallback: Extracting a portion of the document between 500 and 900 words.\n",
            "Processed & saved: /content/extracted_text/Case_32.txt\n",
            "Processing file: Case_33.pdf\n",
            "First extraction (BERT) is less than 500 words. Applying rule-based fallback.\n",
            "Fallback: No key sections found. Extracting first 900 words as fallback.\n",
            "Combined text is still less than 500 words. Applying length-based extraction.\n",
            "Fallback: Extracting a portion of the document between 500 and 900 words.\n",
            "Processed & saved: /content/extracted_text/Case_33.txt\n",
            "Processing file: Case_34.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First extraction (BERT) is less than 500 words. Applying rule-based fallback.\n",
            "Processed & saved: /content/extracted_text/Case_34.txt\n",
            "Processing file: Case_35.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed & saved: /content/extracted_text/Case_35.txt\n",
            "Processing file: Case_36.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed & saved: /content/extracted_text/Case_36.txt\n",
            "Processing file: Case_37.pdf\n",
            "First extraction (BERT) is less than 500 words. Applying rule-based fallback.\n",
            "Fallback: No key sections found. Extracting first 900 words as fallback.\n",
            "Combined text is still less than 500 words. Applying length-based extraction.\n",
            "Fallback: Extracting a portion of the document between 500 and 900 words.\n",
            "Processed & saved: /content/extracted_text/Case_37.txt\n",
            "Processing file: Case_38.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed & saved: /content/extracted_text/Case_38.txt\n",
            "Processing file: Case_39.pdf\n",
            "First extraction (BERT) is less than 500 words. Applying rule-based fallback.\n",
            "Fallback: No key sections found. Extracting first 900 words as fallback.\n",
            "Combined text is still less than 500 words. Applying length-based extraction.\n",
            "Fallback: Extracting a portion of the document between 500 and 900 words.\n",
            "Processed & saved: /content/extracted_text/Case_39.txt\n",
            "Processing file: Case_40.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First extraction (BERT) is less than 500 words. Applying rule-based fallback.\n",
            "Fallback: No key sections found. Extracting first 900 words as fallback.\n",
            "Processed & saved: /content/extracted_text/Case_40.txt\n",
            "Processing file: Case_41.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First extraction (BERT) is less than 500 words. Applying rule-based fallback.\n",
            "Fallback: No key sections found. Extracting first 900 words as fallback.\n",
            "Processed & saved: /content/extracted_text/Case_41.txt\n",
            "Processing file: Case_42.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed & saved: /content/extracted_text/Case_42.txt\n",
            "Processing file: Case_43.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First extraction (BERT) is less than 500 words. Applying rule-based fallback.\n",
            "Fallback: No key sections found. Extracting first 900 words as fallback.\n",
            "Processed & saved: /content/extracted_text/Case_43.txt\n",
            "Processing file: Case_44.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed & saved: /content/extracted_text/Case_44.txt\n",
            "Processing file: Case_45.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed & saved: /content/extracted_text/Case_45.txt\n",
            "Processing file: Case_46.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed & saved: /content/extracted_text/Case_46.txt\n",
            "Processing file: Case_47.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed & saved: /content/extracted_text/Case_47.txt\n",
            "Processing file: Case_48.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed & saved: /content/extracted_text/Case_48.txt\n",
            "Processing file: Case_49.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First extraction (BERT) is less than 500 words. Applying rule-based fallback.\n",
            "Combined text is still less than 500 words. Applying length-based extraction.\n",
            "Fallback: Extracting a portion of the document between 500 and 900 words.\n",
            "Processed & saved: /content/extracted_text/Case_49.txt\n",
            "Processing file: Case_50.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed & saved: /content/extracted_text/Case_50.txt\n",
            "Processing file: Case_51.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed & saved: /content/extracted_text/Case_51.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Post processing of extracted text\n",
        "\n",
        "1.   Grammar Correction using TextBlob model\n",
        "2.   Text cleaing (Removing unwanted characters, IndianKanoon word, tariling full stops after heading, excessive spaces)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NuGP-JG3BAdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Correct grammar using TextBlob (for spelling and punctuation)\n",
        "def correct_grammar(text):\n",
        "    corrected_text = TextBlob(text).correct()\n",
        "    return str(corrected_text)\n",
        "\n",
        "# Clean text by removing unwanted characters (brackets, special chars, etc.)\n",
        "def clean_extracted_text(text):\n",
        "    # Remove unwanted characters like brackets, etc.\n",
        "    text = re.sub(r'[^\\w\\s,.-]', '', text)  # Remove unwanted characters (e.g., brackets)\n",
        "    text = re.sub(r'\\[.*?\\]', '', text)  # Remove anything within square brackets\n",
        "    text = re.sub(r'\\s+', ' ', text.strip())  # Remove excessive spaces\n",
        "    text = re.sub(r'\\s+\\.\\s*$', '', text)  # Remove trailing full stops after headings\n",
        "    text = re.sub(r'IndianKanoon\\s*-\\s*', '', text)  # Remove \"Indian Kanoon\" references\n",
        "\n",
        "    return text\n",
        "\n",
        "# Ensure text meets the word count limits (min 300, max 800 words)\n",
        "def enforce_word_count(text, min_words=300, max_words=800):\n",
        "    word_list = text.split()\n",
        "\n",
        "    # Truncate if too long\n",
        "    if len(word_list) > max_words:\n",
        "        text = ' '.join(word_list[:max_words])\n",
        "    # Ensure text is at least 200 words\n",
        "    elif len(word_list) < min_words:\n",
        "        # We won't add padding text. If the text is less than min_words, just return it as it is.\n",
        "        print(f\"Warning: Text has less than {min_words} words. This may be problematic.\")\n",
        "\n",
        "    return text\n",
        "\n",
        "# Process extracted text and save it\n",
        "def process_and_save_text(input_folder, output_folder):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    all_files = os.listdir(input_folder)\n",
        "\n",
        "    for file in all_files:\n",
        "        # Skip if it's a directory\n",
        "        if os.path.isdir(os.path.join(input_folder, file)):\n",
        "            continue\n",
        "\n",
        "        with open(os.path.join(input_folder, file), 'r') as f:\n",
        "            text = f.read()\n",
        "\n",
        "        # Clean and correct grammar\n",
        "        cleaned_text = clean_extracted_text(text)\n",
        "        corrected_text = correct_grammar(cleaned_text)\n",
        "\n",
        "        # Ensure word count is within range (min 300, max 800)\n",
        "        final_text = enforce_word_count(corrected_text, min_words=300, max_words=800)\n",
        "\n",
        "        # Save processed text\n",
        "        with open(os.path.join(output_folder, file), 'w') as f:\n",
        "            f.write(final_text)\n",
        "        print(f\"Processed & saved: {file}\")\n",
        "\n",
        "# Paths for processed text\n",
        "output_folder = \"/content/postprocessed_text\"\n",
        "\n",
        "# Process the extracted text and save it in processed_text folder, without excluding any files\n",
        "process_and_save_text(\"/content/extracted_text\", output_folder)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9BFxkEX0BPr",
        "outputId": "73f6ef8a-1eb2-4cf8-e2ab-f69a23c7400b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed & saved: Case_49.txt\n",
            "Processed & saved: Case_13.txt\n",
            "Processed & saved: Case_3.txt\n",
            "Processed & saved: Case_10.txt\n",
            "Processed & saved: Case_4.txt\n",
            "Processed & saved: Case_2.txt\n",
            "Processed & saved: Case_41.txt\n",
            "Processed & saved: Case_15.txt\n",
            "Processed & saved: Case_38.txt\n",
            "Processed & saved: Case_27.txt\n",
            "Processed & saved: Case_16.txt\n",
            "Processed & saved: Case_17.txt\n",
            "Processed & saved: Case_19.txt\n",
            "Processed & saved: Case_34.txt\n",
            "Processed & saved: Case_46.txt\n",
            "Processed & saved: Case_36.txt\n",
            "Processed & saved: Case_30.txt\n",
            "Processed & saved: Case_50.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is just to check the file count in a folder"
      ],
      "metadata": {
        "id": "WWh59XqgBp4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def count_files_in_directory(directory_path):\n",
        "    try:\n",
        "        # List all files and directories in the given path\n",
        "        files = [f for f in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, f))]\n",
        "        # Count the number of files\n",
        "        return len(files)\n",
        "    except FileNotFoundError:\n",
        "        return f\"The directory {directory_path} does not exist.\"\n",
        "    except Exception as e:\n",
        "        return str(e)\n",
        "\n",
        "# Replace with the path to your directory\n",
        "directory_path = '/content/drive/MyDrive/Dataset/Case_Files/Final_audio_files'\n",
        "file_count = count_files_in_directory(directory_path)\n",
        "print(f\"Number of files in the directory: {file_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJ2T54xHcEDG",
        "outputId": "b4fa8137-df1a-400e-9635-823c625daa83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of files in the directory: 51\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checks for files that are not in word count range"
      ],
      "metadata": {
        "id": "cdwIYJLoBvr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Function to calculate word count of a file\n",
        "def get_word_count(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        text = f.read()\n",
        "    words = text.split()\n",
        "    return len(words)\n",
        "\n",
        "# Function to check files based on word count\n",
        "def check_files_for_word_count(input_folder, min_words=400, max_words=1000):\n",
        "    files_out_of_range = []\n",
        "\n",
        "    # Get all files in the folder\n",
        "    all_files = os.listdir(input_folder)\n",
        "\n",
        "    for file in all_files:\n",
        "        file_path = os.path.join(input_folder, file)\n",
        "\n",
        "        # Check if it's a text file\n",
        "        if os.path.isfile(file_path) and file.lower().endswith('.txt'):\n",
        "            word_count = get_word_count(file_path)\n",
        "\n",
        "            # Check if the word count is out of the specified range\n",
        "            if word_count < min_words or word_count > max_words:\n",
        "                files_out_of_range.append((file, word_count))\n",
        "\n",
        "    return files_out_of_range\n",
        "\n",
        "# Paths\n",
        "input_folder = \"/content/drive/MyDrive/Dataset/Case_Files/PT\"  # Set to the folder containing the text files\n",
        "\n",
        "# Get files with word count less than 300 or greater than 800\n",
        "files_out_of_range = check_files_for_word_count(input_folder)\n",
        "\n",
        "# Print the result\n",
        "if files_out_of_range:\n",
        "    print(\"Files with word count out of range (less than 300 or more than 800 words):\")\n",
        "    for file, word_count in files_out_of_range:\n",
        "        print(f\"{file}: {word_count} words\")\n",
        "else:\n",
        "    print(\"All files are within the word count range.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKQA1UnbZM80",
        "outputId": "dc08a5c7-76b4-4239-a1ee-08a969edc18d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All files are within the word count range.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers language-tool-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUt4zKdq5W6o",
        "outputId": "3b07ccd5-95d8-4bfc-b424-5a580f93839f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Collecting language-tool-python\n",
            "  Downloading language_tool_python-2.9.0-py3-none-any.whl.metadata (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from language-tool-python) (5.9.5)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.11/dist-packages (from language-tool-python) (0.10.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Downloading language_tool_python-2.9.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: language-tool-python\n",
            "Successfully installed language-tool-python-2.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GTTS Model installation"
      ],
      "metadata": {
        "id": "kYQWsYRGB5do"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gTTS==2.5.4\n",
        "!pip install pydub==0.25.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWDDF0XX6sL6",
        "outputId": "eed4a99e-c14e-43eb-f345-57704cbb4503"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gTTS==2.5.4\n",
            "  Downloading gTTS-2.5.4-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from gTTS==2.5.4) (2.32.3)\n",
            "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.11/dist-packages (from gTTS==2.5.4) (8.1.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS==2.5.4) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS==2.5.4) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS==2.5.4) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS==2.5.4) (2025.1.31)\n",
            "Downloading gTTS-2.5.4-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: gTTS\n",
            "Successfully installed gTTS-2.5.4\n",
            "Collecting pydub==0.25.1\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google Text to sppech conversion of extracted text files"
      ],
      "metadata": {
        "id": "HQREbhIuB1JX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time  # Importing time module for delay\n",
        "from gtts import gTTS\n",
        "from pydub import AudioSegment\n",
        "\n",
        "def generate_audio_from_text_file(processed_text_file_path, output_audio_file_path):\n",
        "    \"\"\"\n",
        "    Generates audio from a text file and saves it as an audio file.\n",
        "    \"\"\"\n",
        "    # Ensure the processed text file exists\n",
        "    if not os.path.exists(processed_text_file_path):\n",
        "        print(f\"Error: File '{processed_text_file_path}' not found.\")\n",
        "        return\n",
        "\n",
        "    # Read the processed text file\n",
        "    with open(processed_text_file_path, 'r', encoding='utf-8') as file:\n",
        "        processed_text_lines = file.readlines()\n",
        "\n",
        "    # Combine all lines into one text block\n",
        "    final_judgment_text = \" \".join([line.strip() for line in processed_text_lines if line.strip()])\n",
        "\n",
        "    if not final_judgment_text:\n",
        "        print(\"Error: No valid text to convert into speech.\")\n",
        "        return\n",
        "\n",
        "    # Convert text to speech\n",
        "    tts = gTTS(text=final_judgment_text, lang=\"en\", slow=False)\n",
        "\n",
        "    # Save the audio as an MP3 file first\n",
        "    temp_mp3 = \"temp_audio.mp3\"\n",
        "    tts.save(temp_mp3)\n",
        "\n",
        "    # Convert MP3 to FLAC using pydub\n",
        "    sound = AudioSegment.from_mp3(temp_mp3)\n",
        "    sound.export(output_audio_file_path, format=\"flac\")\n",
        "    os.remove(temp_mp3)  # Clean up temporary MP3 file\n",
        "\n",
        "    print(f\"Audio saved as '{output_audio_file_path}'\")\n",
        "\n",
        "def process_all_text_files(input_text_folder, output_audio_folder):\n",
        "    \"\"\"\n",
        "    Processes all judgment text files in the input folder and saves the corresponding audio files in the output folder.\n",
        "    \"\"\"\n",
        "    # Check if the output folder exists, if not, create it\n",
        "    os.makedirs(output_audio_folder, exist_ok=True)\n",
        "\n",
        "    # Process all text files in the input folder\n",
        "    all_files = os.listdir(input_text_folder)\n",
        "\n",
        "    for text_file in all_files:\n",
        "        # Only process .txt files\n",
        "        if not text_file.endswith('.txt'):\n",
        "            continue\n",
        "\n",
        "        text_file_path = os.path.join(input_text_folder, text_file)\n",
        "\n",
        "        # Ensure the file exists before proceeding\n",
        "        if not os.path.exists(text_file_path):\n",
        "            print(f\"Error: File '{text_file}' not found in the input folder.\")\n",
        "            continue\n",
        "\n",
        "        # Remove the .txt extension from the text file name\n",
        "        audio_file_name = os.path.splitext(text_file)[0]\n",
        "\n",
        "        # Output audio file path using the same name as the text file\n",
        "        output_audio_file_path = os.path.join(output_audio_folder, f\"{audio_file_name}.flac\")\n",
        "\n",
        "        # Check if the audio file already exists, if yes, skip it\n",
        "        if os.path.exists(output_audio_file_path):\n",
        "            print(f\"Audio for '{text_file}' already exists. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Generate audio from the text file and save it\n",
        "        generate_audio_from_text_file(text_file_path, output_audio_file_path)\n",
        "\n",
        "        # Introduce a 5-second delay after saving each audio file\n",
        "        time.sleep(5)\n",
        "\n",
        "# Path to the input folder containing processed judgment text files\n",
        "input_text_folder = \"/content/drive/MyDrive/Dataset/Case_Files/postprocessed_text\"  # Update with your folder path\n",
        "\n",
        "# Path to the output folder for saving audio files\n",
        "output_audio_folder = \"/content/drive/MyDrive/Dataset/Case_Files/FAF\"  # Update with your folder path\n",
        "\n",
        "# Process all judgment text files and generate audio files\n",
        "process_all_text_files(input_text_folder, output_audio_folder)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2N2hnWPApE6",
        "outputId": "1d1af502-4895-4501-adc0-a69ef7f8133f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Audio saved as '/content/drive/MyDrive/Dataset/Case_Files/FAF/Case_7.flac'\n",
            "Audio saved as '/content/drive/MyDrive/Dataset/Case_Files/FAF/Case_39.flac'\n",
            "Audio saved as '/content/drive/MyDrive/Dataset/Case_Files/FAF/Case_6.flac'\n",
            "Audio saved as '/content/drive/MyDrive/Dataset/Case_Files/FAF/Case_28.flac'\n",
            "Audio saved as '/content/drive/MyDrive/Dataset/Case_Files/FAF/Case_4.flac'\n",
            "Audio for 'Case_17.txt' already exists. Skipping.\n",
            "Audio saved as '/content/drive/MyDrive/Dataset/Case_Files/FAF/Case_19.flac'\n",
            "Audio saved as '/content/drive/MyDrive/Dataset/Case_Files/FAF/Case_30.flac'\n",
            "Audio saved as '/content/drive/MyDrive/Dataset/Case_Files/FAF/Case_1.flac'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculates the whole duration of audio files in a folder"
      ],
      "metadata": {
        "id": "dZsQwnTRCDyO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pydub.utils import mediainfo\n",
        "\n",
        "def get_audio_duration(file_path):\n",
        "    \"\"\"\n",
        "    Returns the duration of the audio file in seconds.\n",
        "    \"\"\"\n",
        "    info = mediainfo(file_path)\n",
        "    return float(info['duration'])\n",
        "\n",
        "def calculate_total_duration(folder_path):\n",
        "    total_duration = 0  # Initialize total duration in seconds\n",
        "\n",
        "    # Loop through all files in the folder\n",
        "    for filename in os.listdir(folder_path):\n",
        "        # Only consider .flac files\n",
        "        if filename.endswith('.flac'):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            total_duration += get_audio_duration(file_path)  # Add duration of this file\n",
        "\n",
        "    return total_duration\n",
        "\n",
        "def convert_seconds_to_hms(total_seconds):\n",
        "    \"\"\"\n",
        "    Converts total seconds to hours, minutes, and seconds.\n",
        "    \"\"\"\n",
        "    hours = total_seconds // 3600\n",
        "    minutes = (total_seconds % 3600) // 60\n",
        "    seconds = total_seconds % 60\n",
        "    return hours, minutes, seconds\n",
        "\n",
        "# Example usage:\n",
        "folder_path = '/content/drive/MyDrive/Dataset/Case_Files/FAF'  # Replace with your folder path\n",
        "total_duration = calculate_total_duration(folder_path)\n",
        "\n",
        "# Convert total duration to hours, minutes, and seconds\n",
        "hours, minutes, seconds = convert_seconds_to_hms(total_duration)\n",
        "\n",
        "print(f\"Total duration of all .flac files: {int(hours)} hours, {int(minutes)} minutes, and {int(seconds)} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuAl9219UCw4",
        "outputId": "029e27cc-f3de-4aa8-82d5-90c030c3a41b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total duration of all .flac files: 5 hours, 20 minutes, and 46 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydub pyvad soundfile librosa\n",
        "!sudo apt-get install ffmpeg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Nh7slwDpOnZn",
        "outputId": "83f307c0-82da-4d44-c713-ad287b04b146"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting pyvad\n",
            "  Downloading pyvad-0.2.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.11/dist-packages (0.13.1)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.10.2.post1)\n",
            "Collecting librosa\n",
            "  Downloading librosa-0.9.2-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from pyvad) (1.26.4)\n",
            "Collecting webrtcvad<3.0.0,>=2.0.10 (from pyvad)\n",
            "  Downloading webrtcvad-2.0.10.tar.gz (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.2/66.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile) (1.17.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Collecting resampy>=0.2.2 (from librosa)\n",
            "  Downloading resampy-0.4.3-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numba>=0.45.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (24.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile) (2.22)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.45.1->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.0->librosa) (4.3.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.0->librosa) (2.32.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.19.1->librosa) (3.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2025.1.31)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading pyvad-0.2.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading librosa-0.9.2-py3-none-any.whl (214 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.3/214.3 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading resampy-0.4.3-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: webrtcvad\n",
            "  Building wheel for webrtcvad (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for webrtcvad: filename=webrtcvad-2.0.10-cp311-cp311-linux_x86_64.whl size=73496 sha256=1dc588124674fcb3db94f295740f87e3a22903c3861e353b96e069f0d74d770a\n",
            "  Stored in directory: /root/.cache/pip/wheels/94/65/3f/292d0b656be33d1c801831201c74b5f68f41a2ae465ff2ee2f\n",
            "Successfully built webrtcvad\n",
            "Installing collected packages: webrtcvad, pydub, resampy, librosa, pyvad\n",
            "  Attempting uninstall: librosa\n",
            "    Found existing installation: librosa 0.10.2.post1\n",
            "    Uninstalling librosa-0.10.2.post1:\n",
            "      Successfully uninstalled librosa-0.10.2.post1\n",
            "Successfully installed librosa-0.9.2 pydub-0.25.1 pyvad-0.2.0 resampy-0.4.3 webrtcvad-2.0.10\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "librosa"
                ]
              },
              "id": "43d6b29601044eb390fc3dac73379a7e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 29 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Whisper Model installation"
      ],
      "metadata": {
        "id": "Arjf1x8ZCPFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install whisper\n",
        "!pip install git+https://github.com/openai/whisper.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3ef8uT5Tfs8",
        "outputId": "4feb18e4-0c9c-40d7-d0af-c0901a25eb35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting whisper\n",
            "  Downloading whisper-1.1.10.tar.gz (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from whisper) (1.17.0)\n",
            "Building wheels for collected packages: whisper\n",
            "  Building wheel for whisper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for whisper: filename=whisper-1.1.10-py3-none-any.whl size=41120 sha256=1a69e0da355a152a42e44ddc0e5a529bcb6ffd5969a2d6d0a40d0a1e74f620b8\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/65/ee/4e6672aabfa486d3341a39a04f8f87c77e5156149299b5a7d0\n",
            "Successfully built whisper\n",
            "Installing collected packages: whisper\n",
            "Successfully installed whisper-1.1.10\n",
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-_4rp8ev_\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-_4rp8ev_\n",
            "  Resolved https://github.com/openai/whisper.git to commit 517a43ecd132a2089d85f4ebc044728a71d49f6e\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (10.6.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (1.26.4)\n",
            "Collecting tiktoken (from openai-whisper==20240930)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (2.5.1+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from triton>=2->openai-whisper==20240930) (3.17.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper==20240930) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20240930) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20240930) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper==20240930) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper==20240930) (3.0.2)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803669 sha256=b8c866830ccc2fa91a17ce9171e371b3310c171cdb8ff4a4a87956cf4ce802cd\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5yt431bd/wheels/1f/1d/98/9583695e6695a6ac0ad42d87511097dce5ba486647dbfecb0e\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 openai-whisper-20240930 tiktoken-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transcribing audio files using Whisper 'Medium' model"
      ],
      "metadata": {
        "id": "AdsfVpuHCSBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import whisper\n",
        "\n",
        "# Load the Whisper model\n",
        "model = whisper.load_model(\"medium\")\n",
        "\n",
        "def transcribe_audio(audio_path):\n",
        "    \"\"\"\n",
        "    Transcribes the audio file using Whisper.\n",
        "    \"\"\"\n",
        "    print(f\"Transcribing: {audio_path}\")\n",
        "    result = model.transcribe(audio_path)\n",
        "    return result['text']\n",
        "\n",
        "def process_audio_files_in_folder(input_folder, output_folder):\n",
        "    \"\"\"\n",
        "    Processes all audio files in the input folder and saves only the transcriptions in the output folder.\n",
        "    \"\"\"\n",
        "    # Check if the output folder exists, if not, create it\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    # Get all audio files in the input folder (filtering for FLAC files)\n",
        "    audio_files = [f for f in os.listdir(input_folder) if f.endswith('.flac')]\n",
        "\n",
        "    # Process each audio file\n",
        "    for audio_file in audio_files:\n",
        "        audio_file_path = os.path.join(input_folder, audio_file)\n",
        "\n",
        "        # Remove the .flac extension from the audio file name to create the output file name\n",
        "        base_name = os.path.splitext(audio_file)[0]  # Remove the extension\n",
        "\n",
        "        # Define the output file path for the transcription\n",
        "        transcription_output_path = os.path.join(output_folder, f\"{base_name}.txt\")\n",
        "\n",
        "        # 1. Transcribe Audio using Whisper\n",
        "        transcribed_text = transcribe_audio(audio_file_path)\n",
        "\n",
        "        # Save the transcription text to the output folder\n",
        "        with open(transcription_output_path, 'w', encoding='utf-8') as file:\n",
        "            file.write(transcribed_text)\n",
        "\n",
        "        print(f\"Processed file {audio_file} and saved transcription in {transcription_output_path}\")\n",
        "\n",
        "# Input folder containing .flac files\n",
        "input_folder = \"/content/drive/MyDrive/Dataset/Case_Files/FAF\"  # Update with your input folder path\n",
        "\n",
        "# Output folder where transcription text files will be saved\n",
        "output_folder = \"/content/drive/MyDrive/Dataset/Case_Files/whisper_text_25Files\"  # Update with your output folder path\n",
        "\n",
        "# Process the audio files and save transcriptions only\n",
        "process_audio_files_in_folder(input_folder, output_folder)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpnnZ-71oqPf",
        "outputId": "e104cde1-c645-4465-b52b-3ee263031be7"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████████████████████████████████| 1.42G/1.42G [00:10<00:00, 151MiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transcribing: /content/drive/MyDrive/Dataset/Case_Files/FAF/Case_33.flac\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed file Case_33.flac and saved transcription in /content/drive/MyDrive/Dataset/Case_Files/whisper_text_25Files/Case_33.txt\n",
            "Transcribing: /content/drive/MyDrive/Dataset/Case_Files/FAF/Case_35.flac\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed file Case_35.flac and saved transcription in /content/drive/MyDrive/Dataset/Case_Files/whisper_text_25Files/Case_35.txt\n",
            "Transcribing: /content/drive/MyDrive/Dataset/Case_Files/FAF/Case_40.flac\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed file Case_40.flac and saved transcription in /content/drive/MyDrive/Dataset/Case_Files/whisper_text_25Files/Case_40.txt\n",
            "Transcribing: /content/drive/MyDrive/Dataset/Case_Files/FAF/Case_9.flac\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed file Case_9.flac and saved transcription in /content/drive/MyDrive/Dataset/Case_Files/whisper_text_25Files/Case_9.txt\n",
            "Transcribing: /content/drive/MyDrive/Dataset/Case_Files/FAF/Case_37.flac\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed file Case_37.flac and saved transcription in /content/drive/MyDrive/Dataset/Case_Files/whisper_text_25Files/Case_37.txt\n",
            "Transcribing: /content/drive/MyDrive/Dataset/Case_Files/FAF/Case_31.flac\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed file Case_31.flac and saved transcription in /content/drive/MyDrive/Dataset/Case_Files/whisper_text_25Files/Case_31.txt\n",
            "Transcribing: /content/drive/MyDrive/Dataset/Case_Files/FAF/Case_44.flac\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed file Case_44.flac and saved transcription in /content/drive/MyDrive/Dataset/Case_Files/whisper_text_25Files/Case_44.txt\n",
            "Transcribing: /content/drive/MyDrive/Dataset/Case_Files/FAF/Case_25.flac\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed file Case_25.flac and saved transcription in /content/drive/MyDrive/Dataset/Case_Files/whisper_text_25Files/Case_25.txt\n",
            "Transcribing: /content/drive/MyDrive/Dataset/Case_Files/FAF/Case_42.flac\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed file Case_42.flac and saved transcription in /content/drive/MyDrive/Dataset/Case_Files/whisper_text_25Files/Case_42.txt\n",
            "Transcribing: /content/drive/MyDrive/Dataset/Case_Files/FAF/Case_24.flac\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed file Case_24.flac and saved transcription in /content/drive/MyDrive/Dataset/Case_Files/whisper_text_25Files/Case_24.txt\n",
            "Transcribing: /content/drive/MyDrive/Dataset/Case_Files/FAF/Case_32.flac\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed file Case_32.flac and saved transcription in /content/drive/MyDrive/Dataset/Case_Files/whisper_text_25Files/Case_32.txt\n",
            "Transcribing: /content/drive/MyDrive/Dataset/Case_Files/FAF/Case_21.flac\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed file Case_21.flac and saved transcription in /content/drive/MyDrive/Dataset/Case_Files/whisper_text_25Files/Case_21.txt\n",
            "Transcribing: /content/drive/MyDrive/Dataset/Case_Files/FAF/Case_18.flac\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed file Case_18.flac and saved transcription in /content/drive/MyDrive/Dataset/Case_Files/whisper_text_25Files/Case_18.txt\n",
            "Transcribing: /content/drive/MyDrive/Dataset/Case_Files/FAF/Case_15.flac\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed file Case_15.flac and saved transcription in /content/drive/MyDrive/Dataset/Case_Files/whisper_text_25Files/Case_15.txt\n",
            "Transcribing: /content/drive/MyDrive/Dataset/Case_Files/FAF/Case_38.flac\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed file Case_38.flac and saved transcription in /content/drive/MyDrive/Dataset/Case_Files/whisper_text_25Files/Case_38.txt\n",
            "Transcribing: /content/drive/MyDrive/Dataset/Case_Files/FAF/Case_27.flac\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed file Case_27.flac and saved transcription in /content/drive/MyDrive/Dataset/Case_Files/whisper_text_25Files/Case_27.txt\n",
            "Transcribing: /content/drive/MyDrive/Dataset/Case_Files/FAF/Case_17.flac\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed file Case_17.flac and saved transcription in /content/drive/MyDrive/Dataset/Case_Files/whisper_text_25Files/Case_17.txt\n",
            "Transcribing: /content/drive/MyDrive/Dataset/Case_Files/FAF/Case_7.flac\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed file Case_7.flac and saved transcription in /content/drive/MyDrive/Dataset/Case_Files/whisper_text_25Files/Case_7.txt\n",
            "Transcribing: /content/drive/MyDrive/Dataset/Case_Files/FAF/Case_39.flac\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed file Case_39.flac and saved transcription in /content/drive/MyDrive/Dataset/Case_Files/whisper_text_25Files/Case_39.txt\n",
            "Transcribing: /content/drive/MyDrive/Dataset/Case_Files/FAF/Case_6.flac\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed file Case_6.flac and saved transcription in /content/drive/MyDrive/Dataset/Case_Files/whisper_text_25Files/Case_6.txt\n",
            "Transcribing: /content/drive/MyDrive/Dataset/Case_Files/FAF/Case_28.flac\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed file Case_28.flac and saved transcription in /content/drive/MyDrive/Dataset/Case_Files/whisper_text_25Files/Case_28.txt\n",
            "Transcribing: /content/drive/MyDrive/Dataset/Case_Files/FAF/Case_4.flac\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed file Case_4.flac and saved transcription in /content/drive/MyDrive/Dataset/Case_Files/whisper_text_25Files/Case_4.txt\n",
            "Transcribing: /content/drive/MyDrive/Dataset/Case_Files/FAF/Case_19.flac\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed file Case_19.flac and saved transcription in /content/drive/MyDrive/Dataset/Case_Files/whisper_text_25Files/Case_19.txt\n",
            "Transcribing: /content/drive/MyDrive/Dataset/Case_Files/FAF/Case_30.flac\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed file Case_30.flac and saved transcription in /content/drive/MyDrive/Dataset/Case_Files/whisper_text_25Files/Case_30.txt\n",
            "Transcribing: /content/drive/MyDrive/Dataset/Case_Files/FAF/Case_1.flac\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed file Case_1.flac and saved transcription in /content/drive/MyDrive/Dataset/Case_Files/whisper_text_25Files/Case_1.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vosk model installation"
      ],
      "metadata": {
        "id": "a2NLYv7FCYp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://alphacephei.com/vosk/models/vosk-model-small-en-us-0.15.zip\n",
        "!unzip vosk-model-small-en-us-0.15.zip -d /content\n",
        "!pip install vosk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yafe_3Sc6TcO",
        "outputId": "688658db-d537-4649-cf49-1965291e270d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-11 05:46:59--  https://alphacephei.com/vosk/models/vosk-model-small-en-us-0.15.zip\n",
            "Resolving alphacephei.com (alphacephei.com)... 188.40.21.16, 2a01:4f8:13a:279f::2\n",
            "Connecting to alphacephei.com (alphacephei.com)|188.40.21.16|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 41205931 (39M) [application/zip]\n",
            "Saving to: ‘vosk-model-small-en-us-0.15.zip’\n",
            "\n",
            "vosk-model-small-en 100%[===================>]  39.30M  21.4MB/s    in 1.8s    \n",
            "\n",
            "2025-03-11 05:47:01 (21.4 MB/s) - ‘vosk-model-small-en-us-0.15.zip’ saved [41205931/41205931]\n",
            "\n",
            "Archive:  vosk-model-small-en-us-0.15.zip\n",
            "   creating: /content/vosk-model-small-en-us-0.15/\n",
            "   creating: /content/vosk-model-small-en-us-0.15/am/\n",
            "  inflating: /content/vosk-model-small-en-us-0.15/am/final.mdl  \n",
            "   creating: /content/vosk-model-small-en-us-0.15/graph/\n",
            "  inflating: /content/vosk-model-small-en-us-0.15/graph/disambig_tid.int  \n",
            "  inflating: /content/vosk-model-small-en-us-0.15/graph/HCLr.fst  \n",
            "  inflating: /content/vosk-model-small-en-us-0.15/graph/Gr.fst  \n",
            "   creating: /content/vosk-model-small-en-us-0.15/graph/phones/\n",
            "  inflating: /content/vosk-model-small-en-us-0.15/graph/phones/word_boundary.int  \n",
            "   creating: /content/vosk-model-small-en-us-0.15/conf/\n",
            "  inflating: /content/vosk-model-small-en-us-0.15/conf/model.conf  \n",
            "  inflating: /content/vosk-model-small-en-us-0.15/conf/mfcc.conf  \n",
            "   creating: /content/vosk-model-small-en-us-0.15/ivector/\n",
            "  inflating: /content/vosk-model-small-en-us-0.15/ivector/splice.conf  \n",
            "  inflating: /content/vosk-model-small-en-us-0.15/ivector/final.dubm  \n",
            "  inflating: /content/vosk-model-small-en-us-0.15/ivector/global_cmvn.stats  \n",
            "  inflating: /content/vosk-model-small-en-us-0.15/ivector/final.ie  \n",
            "  inflating: /content/vosk-model-small-en-us-0.15/ivector/online_cmvn.conf  \n",
            "  inflating: /content/vosk-model-small-en-us-0.15/ivector/final.mat  \n",
            "  inflating: /content/vosk-model-small-en-us-0.15/README  \n",
            "Collecting vosk\n",
            "  Downloading vosk-0.3.45-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from vosk) (1.17.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from vosk) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from vosk) (4.67.1)\n",
            "Collecting srt (from vosk)\n",
            "  Downloading srt-3.5.3.tar.gz (28 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: websockets in /usr/local/lib/python3.11/dist-packages (from vosk) (14.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->vosk) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->vosk) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->vosk) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->vosk) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->vosk) (2025.1.31)\n",
            "Downloading vosk-0.3.45-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: srt\n",
            "  Building wheel for srt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for srt: filename=srt-3.5.3-py3-none-any.whl size=22428 sha256=4f436510018efcd598461e65be026bfb7b5060b5367c5a8a69bc6b1db1a3b7a4\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/43/f1/23ee9119497fcb57d9f7046fbf34c6d9027c46a1fa7824cf08\n",
            "Successfully built srt\n",
            "Installing collected packages: srt, vosk\n",
            "Successfully installed srt-3.5.3 vosk-0.3.45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transcribing audio files using Vosk 'vosk-model-small-en-us-0.15' model"
      ],
      "metadata": {
        "id": "C0Bf5jwuCizA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import wave\n",
        "import json\n",
        "from pydub import AudioSegment\n",
        "from vosk import Model, KaldiRecognizer\n",
        "\n",
        "# Load the Vosk model\n",
        "model = Model(\"vosk-model-small-en-us-0.15\")  # Make sure to provide the correct model path\n",
        "\n",
        "def convert_flac_to_wav(input_file, output_file):\n",
        "    \"\"\"\n",
        "    Converts FLAC audio file to WAV format using pydub.\n",
        "    \"\"\"\n",
        "    audio = AudioSegment.from_file(input_file, format=\"flac\")\n",
        "    audio.export(output_file, format=\"wav\")\n",
        "    print(f\"Converted {input_file} to {output_file}\")\n",
        "    return output_file\n",
        "\n",
        "def transcribe_audio(audio_path):\n",
        "    \"\"\"\n",
        "    Transcribes the audio file using Vosk.\n",
        "    \"\"\"\n",
        "    print(f\"Transcribing: {audio_path}\")\n",
        "\n",
        "    # Open audio file\n",
        "    wf = wave.open(audio_path, \"rb\")\n",
        "    rec = KaldiRecognizer(model, wf.getframerate())\n",
        "\n",
        "    transcription = []\n",
        "\n",
        "    # Read audio and process it\n",
        "    while True:\n",
        "        data = wf.readframes(4000)\n",
        "        if len(data) == 0:\n",
        "            break\n",
        "        if rec.AcceptWaveform(data):\n",
        "            result = json.loads(rec.Result())\n",
        "            transcription.append(result.get('text', ''))\n",
        "\n",
        "    # Final transcription\n",
        "    final_result = json.loads(rec.FinalResult())\n",
        "    transcription.append(final_result.get('text', ''))\n",
        "\n",
        "    return \" \".join(transcription)\n",
        "\n",
        "def process_audio_files_in_folder(input_folder, output_folder):\n",
        "    \"\"\"\n",
        "    Processes the first 10 audio files in the input folder and saves only the transcriptions in the output folder.\n",
        "    \"\"\"\n",
        "    # Check if the output folder exists, if not, create it\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    # Get all audio files in the input folder (filtering for FLAC files)\n",
        "    audio_files = [f for f in os.listdir(input_folder) if f.endswith('.flac')]\n",
        "\n",
        "    # Process each audio file\n",
        "    for audio_file in audio_files:\n",
        "        audio_file_path = os.path.join(input_folder, audio_file)\n",
        "\n",
        "        # Remove the .flac extension from the audio file name to create the output file name\n",
        "        base_name = os.path.splitext(audio_file)[0]  # Remove the extension\n",
        "\n",
        "        # Define the output file path for the WAV conversion\n",
        "        wav_file_path = os.path.join(output_folder, f\"{base_name}.wav\")\n",
        "\n",
        "        # 1. Convert FLAC to WAV\n",
        "        convert_flac_to_wav(audio_file_path, wav_file_path)\n",
        "\n",
        "        # 2. Transcribe Audio using Vosk\n",
        "        transcribed_text = transcribe_audio(wav_file_path)\n",
        "\n",
        "        # Define the output file path for the transcription\n",
        "        transcription_output_path = os.path.join(output_folder, f\"{base_name}.txt\")\n",
        "\n",
        "        # Save the transcription text to the output folder\n",
        "        with open(transcription_output_path, 'w', encoding='utf-8') as file:\n",
        "            file.write(transcribed_text)\n",
        "\n",
        "        print(f\"Processed file {audio_file} and saved transcription in {transcription_output_path}\")\n",
        "\n",
        "# Input folder containing .flac files\n",
        "input_folder = \"/content/drive/MyDrive/Dataset/Case_Files/FAF\"  # Update with your input folder path\n",
        "\n",
        "# Output folder where transcription text files will be saved\n",
        "output_folder = \"/content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files\"  # Update with your output folder path\n",
        "\n",
        "# Process the first 10 audio files and save transcriptions only\n",
        "process_audio_files_in_folder(input_folder, output_folder)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xy0s8c8w6wQ2",
        "outputId": "d089a84f-be0d-4e48-9591-6a9e182cabd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted /content/drive/MyDrive/Dataset/Case_Files/FAF/Case_33.flac to /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_33.wav\n",
            "Transcribing: /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_33.wav\n",
            "Processed file Case_33.flac and saved transcription in /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_33.txt\n",
            "Converted /content/drive/MyDrive/Dataset/Case_Files/FAF/Case_35.flac to /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_35.wav\n",
            "Transcribing: /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_35.wav\n",
            "Processed file Case_35.flac and saved transcription in /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_35.txt\n",
            "Converted /content/drive/MyDrive/Dataset/Case_Files/FAF/Case_40.flac to /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_40.wav\n",
            "Transcribing: /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_40.wav\n",
            "Processed file Case_40.flac and saved transcription in /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_40.txt\n",
            "Converted /content/drive/MyDrive/Dataset/Case_Files/FAF/Case_9.flac to /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_9.wav\n",
            "Transcribing: /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_9.wav\n",
            "Processed file Case_9.flac and saved transcription in /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_9.txt\n",
            "Converted /content/drive/MyDrive/Dataset/Case_Files/FAF/Case_37.flac to /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_37.wav\n",
            "Transcribing: /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_37.wav\n",
            "Processed file Case_37.flac and saved transcription in /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_37.txt\n",
            "Converted /content/drive/MyDrive/Dataset/Case_Files/FAF/Case_31.flac to /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_31.wav\n",
            "Transcribing: /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_31.wav\n",
            "Processed file Case_31.flac and saved transcription in /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_31.txt\n",
            "Converted /content/drive/MyDrive/Dataset/Case_Files/FAF/Case_44.flac to /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_44.wav\n",
            "Transcribing: /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_44.wav\n",
            "Processed file Case_44.flac and saved transcription in /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_44.txt\n",
            "Converted /content/drive/MyDrive/Dataset/Case_Files/FAF/Case_25.flac to /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_25.wav\n",
            "Transcribing: /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_25.wav\n",
            "Processed file Case_25.flac and saved transcription in /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_25.txt\n",
            "Converted /content/drive/MyDrive/Dataset/Case_Files/FAF/Case_42.flac to /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_42.wav\n",
            "Transcribing: /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_42.wav\n",
            "Processed file Case_42.flac and saved transcription in /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_42.txt\n",
            "Converted /content/drive/MyDrive/Dataset/Case_Files/FAF/Case_24.flac to /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_24.wav\n",
            "Transcribing: /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_24.wav\n",
            "Processed file Case_24.flac and saved transcription in /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_24.txt\n",
            "Converted /content/drive/MyDrive/Dataset/Case_Files/FAF/Case_32.flac to /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_32.wav\n",
            "Transcribing: /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_32.wav\n",
            "Processed file Case_32.flac and saved transcription in /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_32.txt\n",
            "Converted /content/drive/MyDrive/Dataset/Case_Files/FAF/Case_21.flac to /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_21.wav\n",
            "Transcribing: /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_21.wav\n",
            "Processed file Case_21.flac and saved transcription in /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_21.txt\n",
            "Converted /content/drive/MyDrive/Dataset/Case_Files/FAF/Case_18.flac to /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_18.wav\n",
            "Transcribing: /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_18.wav\n",
            "Processed file Case_18.flac and saved transcription in /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_18.txt\n",
            "Converted /content/drive/MyDrive/Dataset/Case_Files/FAF/Case_15.flac to /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_15.wav\n",
            "Transcribing: /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_15.wav\n",
            "Processed file Case_15.flac and saved transcription in /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_15.txt\n",
            "Converted /content/drive/MyDrive/Dataset/Case_Files/FAF/Case_38.flac to /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_38.wav\n",
            "Transcribing: /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_38.wav\n",
            "Processed file Case_38.flac and saved transcription in /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_38.txt\n",
            "Converted /content/drive/MyDrive/Dataset/Case_Files/FAF/Case_27.flac to /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_27.wav\n",
            "Transcribing: /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_27.wav\n",
            "Processed file Case_27.flac and saved transcription in /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_27.txt\n",
            "Converted /content/drive/MyDrive/Dataset/Case_Files/FAF/Case_17.flac to /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_17.wav\n",
            "Transcribing: /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_17.wav\n",
            "Processed file Case_17.flac and saved transcription in /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_17.txt\n",
            "Converted /content/drive/MyDrive/Dataset/Case_Files/FAF/Case_7.flac to /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_7.wav\n",
            "Transcribing: /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_7.wav\n",
            "Processed file Case_7.flac and saved transcription in /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_7.txt\n",
            "Converted /content/drive/MyDrive/Dataset/Case_Files/FAF/Case_39.flac to /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_39.wav\n",
            "Transcribing: /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_39.wav\n",
            "Processed file Case_39.flac and saved transcription in /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_39.txt\n",
            "Converted /content/drive/MyDrive/Dataset/Case_Files/FAF/Case_6.flac to /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_6.wav\n",
            "Transcribing: /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_6.wav\n",
            "Processed file Case_6.flac and saved transcription in /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_6.txt\n",
            "Converted /content/drive/MyDrive/Dataset/Case_Files/FAF/Case_28.flac to /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_28.wav\n",
            "Transcribing: /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_28.wav\n",
            "Processed file Case_28.flac and saved transcription in /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_28.txt\n",
            "Converted /content/drive/MyDrive/Dataset/Case_Files/FAF/Case_4.flac to /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_4.wav\n",
            "Transcribing: /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_4.wav\n",
            "Processed file Case_4.flac and saved transcription in /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_4.txt\n",
            "Converted /content/drive/MyDrive/Dataset/Case_Files/FAF/Case_19.flac to /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_19.wav\n",
            "Transcribing: /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_19.wav\n",
            "Processed file Case_19.flac and saved transcription in /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_19.txt\n",
            "Converted /content/drive/MyDrive/Dataset/Case_Files/FAF/Case_30.flac to /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_30.wav\n",
            "Transcribing: /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_30.wav\n",
            "Processed file Case_30.flac and saved transcription in /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_30.txt\n",
            "Converted /content/drive/MyDrive/Dataset/Case_Files/FAF/Case_1.flac to /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_1.wav\n",
            "Transcribing: /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_1.wav\n",
            "Processed file Case_1.flac and saved transcription in /content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_1.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wav2vac2.0 libraries installation"
      ],
      "metadata": {
        "id": "q9IbwhL1CtCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchaudio transformers librosa"
      ],
      "metadata": {
        "id": "4V2rGv2M7zKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transcribing audio files using Wav2vac2.0 model"
      ],
      "metadata": {
        "id": "luknfTRlC0Jf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import warnings\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "import soundfile as sf\n",
        "import librosa\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "import re\n",
        "\n",
        "# Suppress warning for weights initialization issues\n",
        "warnings.filterwarnings(\"ignore\", message=\"Some weights of Wav2Vec2ForCTC were not initialized\")\n",
        "\n",
        "# Load the Wav2Vec 2.0 model and processor\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "\n",
        "# Define the target sampling rate (16kHz for Wav2Vec2)\n",
        "TARGET_SAMPLING_RATE = 16000\n",
        "\n",
        "def transcribe_audio(audio_path):\n",
        "    \"\"\"\n",
        "    Transcribes the audio file using Wav2Vec 2.0.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"Transcribing: {audio_path}\")\n",
        "\n",
        "        # Read the audio file and resample to the required sampling rate\n",
        "        audio_input, original_sr = sf.read(audio_path)\n",
        "        if original_sr != TARGET_SAMPLING_RATE:\n",
        "            audio_input = librosa.resample(audio_input, orig_sr=original_sr, target_sr=TARGET_SAMPLING_RATE)\n",
        "\n",
        "        # Process the audio for the model\n",
        "        inputs = processor(audio_input, return_tensors=\"pt\", sampling_rate=TARGET_SAMPLING_RATE, padding=True)\n",
        "\n",
        "        # Get model predictions\n",
        "        with torch.no_grad():\n",
        "            logits = model(input_values=inputs.input_values).logits\n",
        "\n",
        "        # Get the predicted ids from logits\n",
        "        predicted_ids = torch.argmax(logits, dim=-1)\n",
        "\n",
        "        # Decode the predicted ids to text\n",
        "        transcription = processor.decode(predicted_ids[0])\n",
        "\n",
        "        return transcription\n",
        "    except Exception as e:\n",
        "        print(f\"Error transcribing {audio_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def process_audio_files_in_folder(input_folder, output_folder):\n",
        "    \"\"\"\n",
        "    Processes the audio files in the input folder and saves transcriptions in the output folder.\n",
        "    \"\"\"\n",
        "    # Check if the output folder exists, if not, create it\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    # Get all audio files in the input folder (filtering for FLAC files)\n",
        "    audio_files = [f for f in os.listdir(input_folder) if f.endswith('.flac')]\n",
        "\n",
        "    # Using ProcessPoolExecutor to transcribe files in parallel\n",
        "    with ProcessPoolExecutor(max_workers=1) as executor:  # Reduced max_workers for better stability\n",
        "        # Process each audio file in parallel\n",
        "        futures = []\n",
        "        for audio_file in audio_files:\n",
        "            audio_file_path = os.path.join(input_folder, audio_file)\n",
        "\n",
        "            # Remove the .flac extension from the audio file name to create the output file name\n",
        "            base_name = os.path.splitext(audio_file)[0]  # Remove the extension\n",
        "\n",
        "            # Define the output file path for the transcription\n",
        "            transcription_output_path = os.path.join(output_folder, f\"{base_name}.txt\")\n",
        "\n",
        "            # Submit transcription task to the executor\n",
        "            futures.append(executor.submit(transcribe_and_save, audio_file_path, transcription_output_path))\n",
        "\n",
        "        # Wait for all tasks to complete\n",
        "        for future in futures:\n",
        "            try:\n",
        "                future.result()  # Get the result (or raise any exception)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing file: {e}\")\n",
        "\n",
        "def transcribe_and_save(audio_file_path, transcription_output_path):\n",
        "    \"\"\"\n",
        "    Transcribes an audio file and saves the transcription to a file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 1. Transcribe Audio using Wav2Vec 2.0\n",
        "        transcribed_text = transcribe_audio(audio_file_path)\n",
        "\n",
        "        if transcribed_text:  # Check if transcription was successful\n",
        "            # 2. Convert the transcription to a more readable form (fix case sensitivity)\n",
        "            corrected_text = correct_case(transcribed_text)\n",
        "\n",
        "            # 3. Save the transcription text to the output folder\n",
        "            with open(transcription_output_path, 'w', encoding='utf-8') as file:\n",
        "                file.write(corrected_text)\n",
        "\n",
        "            print(f\"Processed file {audio_file_path} and saved transcription in {transcription_output_path}\")\n",
        "        else:\n",
        "            print(f\"Skipping {audio_file_path} due to transcription error.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error transcribing file {audio_file_path}: {e}\")\n",
        "\n",
        "def correct_case(text):\n",
        "    \"\"\"\n",
        "    Corrects the case of the transcription. This function aims to make the transcription more readable.\n",
        "    It capitalizes the first word of each sentence and leaves the rest in lowercase.\n",
        "    \"\"\"\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Capitalize the first letter of each sentence\n",
        "    text = re.sub(r'([.!?]\\s+|^)([a-z])', lambda match: match.group(1) + match.group(2).upper(), text)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Input folder containing .flac files\n",
        "input_folder = \"/content/drive/MyDrive/Dataset/Case_Files/final_audio_files\"  # Update with your input folder path\n",
        "\n",
        "# Output folder where transcription text files will be saved\n",
        "output_folder = \"/content/wav2vec2.0_transcribed_text\"  # Update with your output folder path\n",
        "\n",
        "# Process the audio files and save transcriptions only\n",
        "process_audio_files_in_folder(input_folder, output_folder)"
      ],
      "metadata": {
        "id": "2eEe_iNGANkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installation for evaluation metrics"
      ],
      "metadata": {
        "id": "JSr0bMa3C9Fv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk editdistance sacrebleu\n",
        "!pip install jiwer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdQUKcohrotC",
        "outputId": "b001c677-819a-450d-f1b8-20b2494dc2c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.11/dist-packages (0.8.1)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Collecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (1.26.4)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.3.1)\n",
            "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.6 portalocker-3.1.1 sacrebleu-2.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing 2 files"
      ],
      "metadata": {
        "id": "w8NBBTv_DAG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import difflib\n",
        "import jiwer\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import numpy as np\n",
        "\n",
        "# Make sure to download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "\n",
        "def compare_files(file1_path, file2_path):\n",
        "    \"\"\"\n",
        "    Compare two transcription text files and calculate:\n",
        "    - Similarity ratio (difflib)\n",
        "    - Word Error Rate (WER)\n",
        "    - Character Error Rate (CER)\n",
        "    - BLEU score\n",
        "    \"\"\"\n",
        "    # Read the contents of both files\n",
        "    with open(file1_path, 'r', encoding='utf-8') as file1:\n",
        "        text1 = file1.read()\n",
        "\n",
        "    with open(file2_path, 'r', encoding='utf-8') as file2:\n",
        "        text2 = file2.read()\n",
        "\n",
        "    # 1. Similarity ratio using difflib\n",
        "    sequence_matcher = difflib.SequenceMatcher(None, text1, text2)\n",
        "    similarity_ratio = sequence_matcher.ratio() * 100\n",
        "    print(f\"Similarity ratio: {similarity_ratio:.2f}%\")\n",
        "\n",
        "    # 2. Word Error Rate (WER) using jiwer\n",
        "    wer = jiwer.wer(text1, text2)\n",
        "    print(f\"Word Error Rate (WER): {wer:.4f}\")\n",
        "\n",
        "    # 3. Character Error Rate (CER)\n",
        "    def cer(reference, hypothesis):\n",
        "        \"\"\"Calculate the Character Error Rate\"\"\"\n",
        "        ref = list(reference.replace(\" \", \"\"))\n",
        "        hyp = list(hypothesis.replace(\" \", \"\"))\n",
        "        distance = np.sum([1 for a, b in zip(ref, hyp) if a != b])\n",
        "        return distance / float(len(ref))\n",
        "\n",
        "    cer_value = cer(text1, text2)\n",
        "    print(f\"Character Error Rate (CER): {cer_value:.4f}\")\n",
        "\n",
        "    # 4. BLEU score (using sentence_bleu from nltk)\n",
        "    reference = text1.split()  # Reference text (split into words)\n",
        "    hypothesis = text2.split()  # Hypothesis text (split into words)\n",
        "    bleu_score = sentence_bleu([reference], hypothesis)  # BLEU score computation\n",
        "    print(f\"BLEU score: {bleu_score:.4f}\")\n",
        "\n",
        "\n",
        "# Example usage\n",
        "file1 = \"/content/drive/MyDrive/Dataset/Case_Files/PT/Case_2.txt\"  # Replace with the path to your first transcription file\n",
        "file2 = \"/content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files/Case_2.txt\"  # Replace with the path to your second transcription file\n",
        "\n",
        "compare_files(file1, file2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0u39Olor3Z_",
        "outputId": "43f86077-fde6-4c17-8fd3-ac9df0ff41e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity ratio: 3.78%\n",
            "Word Error Rate (WER): 0.2300\n",
            "Character Error Rate (CER): 0.9232\n",
            "BLEU score: 0.6239\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing 2 folders (The text is converted to lower text and then compared)(postprocessed text and Wav2vac2.0 transcribed text)"
      ],
      "metadata": {
        "id": "q5HFfNUCDHYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import editdistance\n",
        "import sacrebleu\n",
        "import string\n",
        "\n",
        "# Function to compute Word Error Rate (WER)\n",
        "def compute_wer(reference, hypothesis):\n",
        "    reference_words = reference.split()\n",
        "    hypothesis_words = hypothesis.split()\n",
        "    return editdistance.eval(reference_words, hypothesis_words) / len(reference_words)\n",
        "\n",
        "# Function to compute Character Error Rate (CER)\n",
        "def compute_cer(reference, hypothesis):\n",
        "    return editdistance.eval(reference, hypothesis) / len(reference)\n",
        "\n",
        "# Function to compute BLEU score\n",
        "def compute_bleu(reference, hypothesis):\n",
        "    bleu = sacrebleu.corpus_bleu([hypothesis], [[reference]])\n",
        "    return bleu.score\n",
        "\n",
        "# Function to preprocess the text (remove punctuation and convert to lowercase)\n",
        "def preprocess_text(text):\n",
        "    # Convert text to lowercase and remove punctuation\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
        "    return text\n",
        "\n",
        "# Read the original text and transcribed text\n",
        "def read_text_files(original_file, transcribed_file):\n",
        "    with open(original_file, 'r') as f:\n",
        "        original_text = f.read().strip()\n",
        "\n",
        "    with open(transcribed_file, 'r') as f:\n",
        "        transcribed_text = f.read().strip()\n",
        "\n",
        "    return original_text, transcribed_text\n",
        "\n",
        "# Function to evaluate metrics for a single file\n",
        "def evaluate_single_file(reference, hypothesis):\n",
        "    # Preprocess the reference and hypothesis text\n",
        "    reference = preprocess_text(reference)\n",
        "    hypothesis = preprocess_text(hypothesis)\n",
        "\n",
        "    # Compute WER\n",
        "    wer = compute_wer(reference, hypothesis)\n",
        "\n",
        "    # Compute CER\n",
        "    cer = compute_cer(reference, hypothesis)\n",
        "\n",
        "    # Compute BLEU score\n",
        "    bleu_score = compute_bleu(reference, hypothesis)\n",
        "\n",
        "    return wer, cer, bleu_score\n",
        "\n",
        "# Main function to evaluate metrics for all matching files in two folders\n",
        "def evaluate_metrics(input_folder, output_folder):\n",
        "    # Get all text files in the input folder\n",
        "    input_files = [f for f in os.listdir(input_folder) if f.endswith('.txt')]\n",
        "\n",
        "    # Initialize variables to accumulate total scores\n",
        "    total_wer = 0\n",
        "    total_cer = 0\n",
        "    total_bleu = 0\n",
        "    file_count = 0\n",
        "\n",
        "    # Loop through each file in the input folder\n",
        "    for input_file in input_files:\n",
        "        # Create the corresponding output file path\n",
        "        input_file_path = os.path.join(input_folder, input_file)\n",
        "        output_file_path = os.path.join(output_folder, input_file)\n",
        "\n",
        "        # Check if the corresponding file exists in the output folder\n",
        "        if os.path.exists(output_file_path):\n",
        "            # Read original and transcribed text\n",
        "            reference, hypothesis = read_text_files(input_file_path, output_file_path)\n",
        "\n",
        "            # Print the file name being processed\n",
        "            print(f\"Evaluating file: {input_file}\")\n",
        "\n",
        "            # Evaluate and accumulate metrics for the current file\n",
        "            wer, cer, bleu_score = evaluate_single_file(reference, hypothesis)\n",
        "            total_wer += wer\n",
        "            total_cer += cer\n",
        "            total_bleu += bleu_score\n",
        "            file_count += 1\n",
        "\n",
        "            # Print evaluation metrics for the current file\n",
        "            print(f\"Word Error Rate (WER): {wer * 100:.2f}%\")\n",
        "            print(f\"Character Error Rate (CER): {cer * 100:.2f}%\")\n",
        "            print(f\"BLEU score: {bleu_score:.2f}\")\n",
        "            print(\"-\" * 50)\n",
        "        else:\n",
        "            print(f\"Warning: The file '{input_file}' is missing in the output folder.\")\n",
        "\n",
        "    # Calculate average metrics\n",
        "    if file_count > 0:\n",
        "        avg_wer = total_wer / file_count\n",
        "        avg_cer = total_cer / file_count\n",
        "        avg_bleu = total_bleu / file_count\n",
        "\n",
        "        # Print average results\n",
        "        print(\"\\nAverage Metrics Across All Files:\")\n",
        "        print(f\"Average Word Error Rate (WER): {avg_wer * 100:.2f}%\")\n",
        "        print(f\"Average Character Error Rate (CER): {avg_cer * 100:.2f}%\")\n",
        "        print(f\"Average BLEU score: {avg_bleu:.2f}\")\n",
        "    else:\n",
        "        print(\"No files were processed.\")\n",
        "\n",
        "# Example usage:\n",
        "# Replace '/path/to/extracted_text' and '/path/to/transcribed_text' with actual folder paths\n",
        "input_folder = '/content/drive/MyDrive/Dataset/Case_Files/PT'  # Folder with original extracted texts\n",
        "output_folder = '/content/drive/MyDrive/Dataset/Case_Files/wav2vac2.0_text_25Files'  # Folder with transcribed texts\n",
        "\n",
        "evaluate_metrics(input_folder, output_folder)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbKmLU3JEORf",
        "outputId": "b7aff30d-5262-419c-b6be-44e98e9f4d06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating file: Case_9.txt\n",
            "Word Error Rate (WER): 79.90%\n",
            "Character Error Rate (CER): 32.75%\n",
            "BLEU score: 15.99\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_37.txt\n",
            "Word Error Rate (WER): 97.54%\n",
            "Character Error Rate (CER): 61.58%\n",
            "BLEU score: 20.16\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_31.txt\n",
            "Word Error Rate (WER): 53.11%\n",
            "Character Error Rate (CER): 22.72%\n",
            "BLEU score: 29.24\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_44.txt\n",
            "Word Error Rate (WER): 63.99%\n",
            "Character Error Rate (CER): 34.94%\n",
            "BLEU score: 23.75\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_25.txt\n",
            "Word Error Rate (WER): 34.09%\n",
            "Character Error Rate (CER): 16.87%\n",
            "BLEU score: 51.53\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_42.txt\n",
            "Word Error Rate (WER): 53.25%\n",
            "Character Error Rate (CER): 29.33%\n",
            "BLEU score: 27.92\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_24.txt\n",
            "Word Error Rate (WER): 88.65%\n",
            "Character Error Rate (CER): 47.77%\n",
            "BLEU score: 12.31\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_32.txt\n",
            "Word Error Rate (WER): 83.42%\n",
            "Character Error Rate (CER): 44.12%\n",
            "BLEU score: 17.02\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_21.txt\n",
            "Word Error Rate (WER): 89.40%\n",
            "Character Error Rate (CER): 56.46%\n",
            "BLEU score: 23.92\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_15.txt\n",
            "Word Error Rate (WER): 20.70%\n",
            "Character Error Rate (CER): 4.89%\n",
            "BLEU score: 64.73\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_38.txt\n",
            "Word Error Rate (WER): 59.60%\n",
            "Character Error Rate (CER): 26.53%\n",
            "BLEU score: 25.38\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_27.txt\n",
            "Word Error Rate (WER): 79.92%\n",
            "Character Error Rate (CER): 50.67%\n",
            "BLEU score: 13.80\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_17.txt\n",
            "Word Error Rate (WER): 54.22%\n",
            "Character Error Rate (CER): 31.34%\n",
            "BLEU score: 31.59\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_19.txt\n",
            "Word Error Rate (WER): 69.81%\n",
            "Character Error Rate (CER): 44.31%\n",
            "BLEU score: 25.18\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_18.txt\n",
            "Word Error Rate (WER): 50.06%\n",
            "Character Error Rate (CER): 20.60%\n",
            "BLEU score: 29.41\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_35.txt\n",
            "Word Error Rate (WER): 69.74%\n",
            "Character Error Rate (CER): 43.08%\n",
            "BLEU score: 21.81\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_40.txt\n",
            "Word Error Rate (WER): 63.85%\n",
            "Character Error Rate (CER): 30.14%\n",
            "BLEU score: 26.40\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_45.txt\n",
            "Word Error Rate (WER): 43.13%\n",
            "Character Error Rate (CER): 22.80%\n",
            "BLEU score: 38.23\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_11.txt\n",
            "Word Error Rate (WER): 35.16%\n",
            "Character Error Rate (CER): 21.42%\n",
            "BLEU score: 53.55\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_47.txt\n",
            "Word Error Rate (WER): 61.51%\n",
            "Character Error Rate (CER): 35.69%\n",
            "BLEU score: 28.75\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_12.txt\n",
            "Word Error Rate (WER): 86.26%\n",
            "Character Error Rate (CER): 41.73%\n",
            "BLEU score: 14.52\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_14.txt\n",
            "Word Error Rate (WER): 68.39%\n",
            "Character Error Rate (CER): 37.48%\n",
            "BLEU score: 21.90\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_8.txt\n",
            "Word Error Rate (WER): 81.89%\n",
            "Character Error Rate (CER): 44.16%\n",
            "BLEU score: 18.07\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_22.txt\n",
            "Word Error Rate (WER): 52.14%\n",
            "Character Error Rate (CER): 24.58%\n",
            "BLEU score: 34.78\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_7.txt\n",
            "Word Error Rate (WER): 98.36%\n",
            "Character Error Rate (CER): 47.55%\n",
            "BLEU score: 11.53\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_48.txt\n",
            "Word Error Rate (WER): 49.55%\n",
            "Character Error Rate (CER): 24.67%\n",
            "BLEU score: 32.90\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_39.txt\n",
            "Word Error Rate (WER): 72.04%\n",
            "Character Error Rate (CER): 50.25%\n",
            "BLEU score: 31.04\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_51.txt\n",
            "Word Error Rate (WER): 43.20%\n",
            "Character Error Rate (CER): 20.63%\n",
            "BLEU score: 39.21\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_5.txt\n",
            "Word Error Rate (WER): 60.95%\n",
            "Character Error Rate (CER): 28.62%\n",
            "BLEU score: 25.52\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_26.txt\n",
            "Word Error Rate (WER): 50.69%\n",
            "Character Error Rate (CER): 24.34%\n",
            "BLEU score: 32.03\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_6.txt\n",
            "Word Error Rate (WER): 86.29%\n",
            "Character Error Rate (CER): 50.48%\n",
            "BLEU score: 15.96\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_23.txt\n",
            "Word Error Rate (WER): 92.11%\n",
            "Character Error Rate (CER): 55.76%\n",
            "BLEU score: 15.17\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_20.txt\n",
            "Word Error Rate (WER): 32.65%\n",
            "Character Error Rate (CER): 13.63%\n",
            "BLEU score: 45.66\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_28.txt\n",
            "Word Error Rate (WER): 60.66%\n",
            "Character Error Rate (CER): 39.21%\n",
            "BLEU score: 27.47\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_29.txt\n",
            "Word Error Rate (WER): 80.18%\n",
            "Character Error Rate (CER): 41.47%\n",
            "BLEU score: 17.41\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_43.txt\n",
            "Word Error Rate (WER): 63.35%\n",
            "Character Error Rate (CER): 34.25%\n",
            "BLEU score: 22.36\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_49.txt\n",
            "Word Error Rate (WER): 92.24%\n",
            "Character Error Rate (CER): 47.37%\n",
            "BLEU score: 16.66\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_13.txt\n",
            "Word Error Rate (WER): 56.30%\n",
            "Character Error Rate (CER): 26.79%\n",
            "BLEU score: 32.33\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_3.txt\n",
            "Word Error Rate (WER): 54.26%\n",
            "Character Error Rate (CER): 21.36%\n",
            "BLEU score: 23.03\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_4.txt\n",
            "Word Error Rate (WER): 50.57%\n",
            "Character Error Rate (CER): 34.76%\n",
            "BLEU score: 43.62\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_2.txt\n",
            "Word Error Rate (WER): 32.91%\n",
            "Character Error Rate (CER): 13.35%\n",
            "BLEU score: 50.01\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_41.txt\n",
            "Word Error Rate (WER): 49.43%\n",
            "Character Error Rate (CER): 21.66%\n",
            "BLEU score: 31.05\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_16.txt\n",
            "Word Error Rate (WER): 99.48%\n",
            "Character Error Rate (CER): 50.24%\n",
            "BLEU score: 10.56\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_34.txt\n",
            "Word Error Rate (WER): 40.26%\n",
            "Character Error Rate (CER): 16.68%\n",
            "BLEU score: 40.00\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_46.txt\n",
            "Word Error Rate (WER): 39.06%\n",
            "Character Error Rate (CER): 21.76%\n",
            "BLEU score: 43.65\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_36.txt\n",
            "Word Error Rate (WER): 38.76%\n",
            "Character Error Rate (CER): 14.64%\n",
            "BLEU score: 41.87\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_30.txt\n",
            "Word Error Rate (WER): 62.44%\n",
            "Character Error Rate (CER): 38.21%\n",
            "BLEU score: 34.08\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_50.txt\n",
            "Word Error Rate (WER): 60.29%\n",
            "Character Error Rate (CER): 28.13%\n",
            "BLEU score: 23.75\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_10.txt\n",
            "Word Error Rate (WER): 68.01%\n",
            "Character Error Rate (CER): 33.21%\n",
            "BLEU score: 28.05\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_1.txt\n",
            "Word Error Rate (WER): 62.28%\n",
            "Character Error Rate (CER): 41.37%\n",
            "BLEU score: 30.24\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_33.txt\n",
            "Word Error Rate (WER): 80.14%\n",
            "Character Error Rate (CER): 40.12%\n",
            "BLEU score: 23.25\n",
            "--------------------------------------------------\n",
            "\n",
            "Average Metrics Across All Files:\n",
            "Average Word Error Rate (WER): 63.06%\n",
            "Average Character Error Rate (CER): 33.46%\n",
            "Average BLEU score: 28.59\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing 2 folders (The text is converted to lower text and then compared)(postprocessed text and whisper transcribed text)"
      ],
      "metadata": {
        "id": "5OjVdh8wDaw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import editdistance\n",
        "import sacrebleu\n",
        "import string\n",
        "\n",
        "# Function to compute Word Error Rate (WER)\n",
        "def compute_wer(reference, hypothesis):\n",
        "    reference_words = reference.split()\n",
        "    hypothesis_words = hypothesis.split()\n",
        "    return editdistance.eval(reference_words, hypothesis_words) / len(reference_words)\n",
        "\n",
        "# Function to compute Character Error Rate (CER)\n",
        "def compute_cer(reference, hypothesis):\n",
        "    return editdistance.eval(reference, hypothesis) / len(reference)\n",
        "\n",
        "# Function to compute BLEU score\n",
        "def compute_bleu(reference, hypothesis):\n",
        "    bleu = sacrebleu.corpus_bleu([hypothesis], [[reference]])\n",
        "    return bleu.score\n",
        "\n",
        "# Function to preprocess the text (remove punctuation and convert to lowercase)\n",
        "def preprocess_text(text):\n",
        "    # Convert text to lowercase and remove punctuation\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
        "    return text\n",
        "\n",
        "# Read the original text and transcribed text\n",
        "def read_text_files(original_file, transcribed_file):\n",
        "    with open(original_file, 'r') as f:\n",
        "        original_text = f.read().strip()\n",
        "\n",
        "    with open(transcribed_file, 'r') as f:\n",
        "        transcribed_text = f.read().strip()\n",
        "\n",
        "    return original_text, transcribed_text\n",
        "\n",
        "# Function to evaluate metrics for a single file\n",
        "def evaluate_single_file(reference, hypothesis):\n",
        "    # Preprocess the reference and hypothesis text\n",
        "    reference = preprocess_text(reference)\n",
        "    hypothesis = preprocess_text(hypothesis)\n",
        "\n",
        "    # Compute WER\n",
        "    wer = compute_wer(reference, hypothesis)\n",
        "\n",
        "    # Compute CER\n",
        "    cer = compute_cer(reference, hypothesis)\n",
        "\n",
        "    # Compute BLEU score\n",
        "    bleu_score = compute_bleu(reference, hypothesis)\n",
        "\n",
        "    return wer, cer, bleu_score\n",
        "\n",
        "# Main function to evaluate metrics for all matching files in two folders\n",
        "def evaluate_metrics(input_folder, output_folder):\n",
        "    # Get all text files in the input folder\n",
        "    input_files = [f for f in os.listdir(input_folder) if f.endswith('.txt')]\n",
        "\n",
        "    # Initialize variables to accumulate total scores\n",
        "    total_wer = 0\n",
        "    total_cer = 0\n",
        "    total_bleu = 0\n",
        "    file_count = 0\n",
        "\n",
        "    # Loop through each file in the input folder\n",
        "    for input_file in input_files:\n",
        "        # Create the corresponding output file path\n",
        "        input_file_path = os.path.join(input_folder, input_file)\n",
        "        output_file_path = os.path.join(output_folder, input_file)\n",
        "\n",
        "        # Check if the corresponding file exists in the output folder\n",
        "        if os.path.exists(output_file_path):\n",
        "            # Read original and transcribed text\n",
        "            reference, hypothesis = read_text_files(input_file_path, output_file_path)\n",
        "\n",
        "            # Print the file name being processed\n",
        "            print(f\"Evaluating file: {input_file}\")\n",
        "\n",
        "            # Evaluate and accumulate metrics for the current file\n",
        "            wer, cer, bleu_score = evaluate_single_file(reference, hypothesis)\n",
        "            total_wer += wer\n",
        "            total_cer += cer\n",
        "            total_bleu += bleu_score\n",
        "            file_count += 1\n",
        "\n",
        "            # Print evaluation metrics for the current file\n",
        "            print(f\"Word Error Rate (WER): {wer * 100:.2f}%\")\n",
        "            print(f\"Character Error Rate (CER): {cer * 100:.2f}%\")\n",
        "            print(f\"BLEU score: {bleu_score:.2f}\")\n",
        "            print(\"-\" * 50)\n",
        "        else:\n",
        "            print(f\"Warning: The file '{input_file}' is missing in the output folder.\")\n",
        "\n",
        "    # Calculate average metrics\n",
        "    if file_count > 0:\n",
        "        avg_wer = total_wer / file_count\n",
        "        avg_cer = total_cer / file_count\n",
        "        avg_bleu = total_bleu / file_count\n",
        "\n",
        "        # Print average results\n",
        "        print(\"\\nAverage Metrics Across All Files:\")\n",
        "        print(f\"Average Word Error Rate (WER): {avg_wer * 100:.2f}%\")\n",
        "        print(f\"Average Character Error Rate (CER): {avg_cer * 100:.2f}%\")\n",
        "        print(f\"Average BLEU score: {avg_bleu:.2f}\")\n",
        "    else:\n",
        "        print(\"No files were processed.\")\n",
        "\n",
        "# Example usage:\n",
        "# Replace '/path/to/extracted_text' and '/path/to/transcribed_text' with actual folder paths\n",
        "input_folder = '/content/drive/MyDrive/Dataset/Case_Files/PT'  # Folder with original extracted texts\n",
        "output_folder = '/content/drive/MyDrive/Dataset/Case_Files/whisper_text_25Files'  # Folder with transcribed texts\n",
        "\n",
        "evaluate_metrics(input_folder, output_folder)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MDe1nEiBU9A",
        "outputId": "4936b17b-47d7-47d7-e3dc-71c4a54ae878"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating file: Case_9.txt\n",
            "Word Error Rate (WER): 20.73%\n",
            "Character Error Rate (CER): 8.78%\n",
            "BLEU score: 70.78\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_37.txt\n",
            "Word Error Rate (WER): 23.77%\n",
            "Character Error Rate (CER): 7.52%\n",
            "BLEU score: 60.46\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_31.txt\n",
            "Word Error Rate (WER): 4.94%\n",
            "Character Error Rate (CER): 1.41%\n",
            "BLEU score: 91.26\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_44.txt\n",
            "Word Error Rate (WER): 8.78%\n",
            "Character Error Rate (CER): 1.89%\n",
            "BLEU score: 85.11\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_25.txt\n",
            "Word Error Rate (WER): 3.08%\n",
            "Character Error Rate (CER): 0.85%\n",
            "BLEU score: 94.14\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_42.txt\n",
            "Word Error Rate (WER): 6.24%\n",
            "Character Error Rate (CER): 1.49%\n",
            "BLEU score: 88.21\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_24.txt\n",
            "Word Error Rate (WER): 14.88%\n",
            "Character Error Rate (CER): 3.92%\n",
            "BLEU score: 78.76\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_32.txt\n",
            "Word Error Rate (WER): 15.19%\n",
            "Character Error Rate (CER): 4.59%\n",
            "BLEU score: 75.91\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_21.txt\n",
            "Word Error Rate (WER): 25.83%\n",
            "Character Error Rate (CER): 13.26%\n",
            "BLEU score: 66.73\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_15.txt\n",
            "Word Error Rate (WER): 6.73%\n",
            "Character Error Rate (CER): 1.69%\n",
            "BLEU score: 89.76\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_38.txt\n",
            "Word Error Rate (WER): 10.33%\n",
            "Character Error Rate (CER): 3.94%\n",
            "BLEU score: 82.27\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_27.txt\n",
            "Word Error Rate (WER): 16.50%\n",
            "Character Error Rate (CER): 4.66%\n",
            "BLEU score: 68.47\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_17.txt\n",
            "Word Error Rate (WER): 21.40%\n",
            "Character Error Rate (CER): 4.35%\n",
            "BLEU score: 67.32\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_19.txt\n",
            "Word Error Rate (WER): 20.75%\n",
            "Character Error Rate (CER): 8.44%\n",
            "BLEU score: 67.79\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_18.txt\n",
            "Word Error Rate (WER): 5.10%\n",
            "Character Error Rate (CER): 1.08%\n",
            "BLEU score: 89.96\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_35.txt\n",
            "Word Error Rate (WER): 24.03%\n",
            "Character Error Rate (CER): 5.29%\n",
            "BLEU score: 63.95\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_40.txt\n",
            "Word Error Rate (WER): 12.59%\n",
            "Character Error Rate (CER): 3.86%\n",
            "BLEU score: 80.89\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_45.txt\n",
            "Word Error Rate (WER): 9.21%\n",
            "Character Error Rate (CER): 4.73%\n",
            "BLEU score: 85.97\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_11.txt\n",
            "Word Error Rate (WER): 9.71%\n",
            "Character Error Rate (CER): 2.49%\n",
            "BLEU score: 86.03\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_47.txt\n",
            "Word Error Rate (WER): 16.48%\n",
            "Character Error Rate (CER): 15.04%\n",
            "BLEU score: 77.67\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_12.txt\n",
            "Word Error Rate (WER): 18.10%\n",
            "Character Error Rate (CER): 8.69%\n",
            "BLEU score: 73.53\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_14.txt\n",
            "Word Error Rate (WER): 19.72%\n",
            "Character Error Rate (CER): 7.58%\n",
            "BLEU score: 75.88\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_8.txt\n",
            "Word Error Rate (WER): 18.36%\n",
            "Character Error Rate (CER): 4.68%\n",
            "BLEU score: 70.67\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_22.txt\n",
            "Word Error Rate (WER): 21.87%\n",
            "Character Error Rate (CER): 9.01%\n",
            "BLEU score: 69.93\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_7.txt\n",
            "Word Error Rate (WER): 24.56%\n",
            "Character Error Rate (CER): 7.60%\n",
            "BLEU score: 64.91\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_48.txt\n",
            "Word Error Rate (WER): 10.01%\n",
            "Character Error Rate (CER): 2.72%\n",
            "BLEU score: 84.60\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_39.txt\n",
            "Word Error Rate (WER): 30.51%\n",
            "Character Error Rate (CER): 14.15%\n",
            "BLEU score: 61.35\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_51.txt\n",
            "Word Error Rate (WER): 7.53%\n",
            "Character Error Rate (CER): 1.49%\n",
            "BLEU score: 86.32\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_5.txt\n",
            "Word Error Rate (WER): 8.26%\n",
            "Character Error Rate (CER): 3.12%\n",
            "BLEU score: 86.00\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_26.txt\n",
            "Word Error Rate (WER): 16.77%\n",
            "Character Error Rate (CER): 7.52%\n",
            "BLEU score: 76.87\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_6.txt\n",
            "Word Error Rate (WER): 27.55%\n",
            "Character Error Rate (CER): 11.70%\n",
            "BLEU score: 62.61\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_23.txt\n",
            "Word Error Rate (WER): 32.82%\n",
            "Character Error Rate (CER): 10.14%\n",
            "BLEU score: 59.91\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_20.txt\n",
            "Word Error Rate (WER): 3.10%\n",
            "Character Error Rate (CER): 2.19%\n",
            "BLEU score: 93.53\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_28.txt\n",
            "Word Error Rate (WER): 16.42%\n",
            "Character Error Rate (CER): 5.71%\n",
            "BLEU score: 73.62\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_29.txt\n",
            "Word Error Rate (WER): 17.53%\n",
            "Character Error Rate (CER): 6.37%\n",
            "BLEU score: 73.82\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_43.txt\n",
            "Word Error Rate (WER): 24.56%\n",
            "Character Error Rate (CER): 14.34%\n",
            "BLEU score: 66.52\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_49.txt\n",
            "Word Error Rate (WER): 27.91%\n",
            "Character Error Rate (CER): 15.07%\n",
            "BLEU score: 67.33\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_13.txt\n",
            "Word Error Rate (WER): 14.86%\n",
            "Character Error Rate (CER): 7.28%\n",
            "BLEU score: 78.59\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_3.txt\n",
            "Word Error Rate (WER): 10.04%\n",
            "Character Error Rate (CER): 1.84%\n",
            "BLEU score: 84.63\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_4.txt\n",
            "Word Error Rate (WER): 26.98%\n",
            "Character Error Rate (CER): 11.78%\n",
            "BLEU score: 64.60\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_2.txt\n",
            "Word Error Rate (WER): 5.03%\n",
            "Character Error Rate (CER): 1.10%\n",
            "BLEU score: 91.38\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_41.txt\n",
            "Word Error Rate (WER): 17.99%\n",
            "Character Error Rate (CER): 7.43%\n",
            "BLEU score: 70.64\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_16.txt\n",
            "Word Error Rate (WER): 20.47%\n",
            "Character Error Rate (CER): 5.60%\n",
            "BLEU score: 70.94\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_34.txt\n",
            "Word Error Rate (WER): 9.00%\n",
            "Character Error Rate (CER): 2.24%\n",
            "BLEU score: 84.92\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_46.txt\n",
            "Word Error Rate (WER): 8.01%\n",
            "Character Error Rate (CER): 1.97%\n",
            "BLEU score: 83.03\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_36.txt\n",
            "Word Error Rate (WER): 8.96%\n",
            "Character Error Rate (CER): 2.84%\n",
            "BLEU score: 82.84\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_30.txt\n",
            "Word Error Rate (WER): 16.67%\n",
            "Character Error Rate (CER): 6.09%\n",
            "BLEU score: 71.68\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_50.txt\n",
            "Word Error Rate (WER): 22.46%\n",
            "Character Error Rate (CER): 6.12%\n",
            "BLEU score: 65.08\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_10.txt\n",
            "Word Error Rate (WER): 10.79%\n",
            "Character Error Rate (CER): 3.87%\n",
            "BLEU score: 83.47\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_1.txt\n",
            "Word Error Rate (WER): 25.44%\n",
            "Character Error Rate (CER): 9.05%\n",
            "BLEU score: 61.99\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_33.txt\n",
            "Word Error Rate (WER): 21.12%\n",
            "Character Error Rate (CER): 6.89%\n",
            "BLEU score: 64.41\n",
            "--------------------------------------------------\n",
            "\n",
            "Average Metrics Across All Files:\n",
            "Average Word Error Rate (WER): 16.07%\n",
            "Average Character Error Rate (CER): 5.99%\n",
            "Average BLEU score: 76.02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing 2 folders (The text is converted to lower text and then compared)(postprocessed text and Vosk transcribed text)"
      ],
      "metadata": {
        "id": "61tP4VGPDlG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import editdistance\n",
        "import sacrebleu\n",
        "import string\n",
        "\n",
        "# Function to compute Word Error Rate (WER)\n",
        "def compute_wer(reference, hypothesis):\n",
        "    reference_words = reference.split()\n",
        "    hypothesis_words = hypothesis.split()\n",
        "    return editdistance.eval(reference_words, hypothesis_words) / len(reference_words)\n",
        "\n",
        "# Function to compute Character Error Rate (CER)\n",
        "def compute_cer(reference, hypothesis):\n",
        "    return editdistance.eval(reference, hypothesis) / len(reference)\n",
        "\n",
        "# Function to compute BLEU score\n",
        "def compute_bleu(reference, hypothesis):\n",
        "    bleu = sacrebleu.corpus_bleu([hypothesis], [[reference]])\n",
        "    return bleu.score\n",
        "\n",
        "# Function to preprocess the text (remove punctuation and convert to lowercase)\n",
        "def preprocess_text(text):\n",
        "    # Convert text to lowercase and remove punctuation\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
        "    return text\n",
        "\n",
        "# Read the original text and transcribed text\n",
        "def read_text_files(original_file, transcribed_file):\n",
        "    with open(original_file, 'r') as f:\n",
        "        original_text = f.read().strip()\n",
        "\n",
        "    with open(transcribed_file, 'r') as f:\n",
        "        transcribed_text = f.read().strip()\n",
        "\n",
        "    return original_text, transcribed_text\n",
        "\n",
        "# Function to evaluate metrics for a single file\n",
        "def evaluate_single_file(reference, hypothesis):\n",
        "    # Preprocess the reference and hypothesis text\n",
        "    reference = preprocess_text(reference)\n",
        "    hypothesis = preprocess_text(hypothesis)\n",
        "\n",
        "    # Compute WER\n",
        "    wer = compute_wer(reference, hypothesis)\n",
        "\n",
        "    # Compute CER\n",
        "    cer = compute_cer(reference, hypothesis)\n",
        "\n",
        "    # Compute BLEU score\n",
        "    bleu_score = compute_bleu(reference, hypothesis)\n",
        "\n",
        "    return wer, cer, bleu_score\n",
        "\n",
        "# Main function to evaluate metrics for all matching files in two folders\n",
        "def evaluate_metrics(input_folder, output_folder):\n",
        "    # Get all text files in the input folder\n",
        "    input_files = [f for f in os.listdir(input_folder) if f.endswith('.txt')]\n",
        "\n",
        "    # Initialize variables to accumulate total scores\n",
        "    total_wer = 0\n",
        "    total_cer = 0\n",
        "    total_bleu = 0\n",
        "    file_count = 0\n",
        "\n",
        "    # Loop through each file in the input folder\n",
        "    for input_file in input_files:\n",
        "        # Create the corresponding output file path\n",
        "        input_file_path = os.path.join(input_folder, input_file)\n",
        "        output_file_path = os.path.join(output_folder, input_file)\n",
        "\n",
        "        # Check if the corresponding file exists in the output folder\n",
        "        if os.path.exists(output_file_path):\n",
        "            # Read original and transcribed text\n",
        "            reference, hypothesis = read_text_files(input_file_path, output_file_path)\n",
        "\n",
        "            # Print the file name being processed\n",
        "            print(f\"Evaluating file: {input_file}\")\n",
        "\n",
        "            # Evaluate and accumulate metrics for the current file\n",
        "            wer, cer, bleu_score = evaluate_single_file(reference, hypothesis)\n",
        "            total_wer += wer\n",
        "            total_cer += cer\n",
        "            total_bleu += bleu_score\n",
        "            file_count += 1\n",
        "\n",
        "            # Print evaluation metrics for the current file\n",
        "            print(f\"Word Error Rate (WER): {wer * 100:.2f}%\")\n",
        "            print(f\"Character Error Rate (CER): {cer * 100:.2f}%\")\n",
        "            print(f\"BLEU score: {bleu_score:.2f}\")\n",
        "            print(\"-\" * 50)\n",
        "        else:\n",
        "            print(f\"Warning: The file '{input_file}' is missing in the output folder.\")\n",
        "\n",
        "    # Calculate average metrics\n",
        "    if file_count > 0:\n",
        "        avg_wer = total_wer / file_count\n",
        "        avg_cer = total_cer / file_count\n",
        "        avg_bleu = total_bleu / file_count\n",
        "\n",
        "        # Print average results\n",
        "        print(\"\\nAverage Metrics Across All Files:\")\n",
        "        print(f\"Average Word Error Rate (WER): {avg_wer * 100:.2f}%\")\n",
        "        print(f\"Average Character Error Rate (CER): {avg_cer * 100:.2f}%\")\n",
        "        print(f\"Average BLEU score: {avg_bleu:.2f}\")\n",
        "    else:\n",
        "        print(\"No files were processed.\")\n",
        "\n",
        "# Example usage:\n",
        "# Replace '/path/to/extracted_text' and '/path/to/transcribed_text' with actual folder paths\n",
        "input_folder = '/content/drive/MyDrive/Dataset/Case_Files/PT'  # Folder with original extracted texts\n",
        "output_folder = '/content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files'  # Folder with transcribed texts\n",
        "\n",
        "evaluate_metrics(input_folder, output_folder)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oq0rmH91EC5K",
        "outputId": "e09835e6-eece-45ff-b413-04b8e454f0b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating file: Case_9.txt\n",
            "Word Error Rate (WER): 56.26%\n",
            "Character Error Rate (CER): 28.37%\n",
            "BLEU score: 45.28\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_37.txt\n",
            "Word Error Rate (WER): 84.43%\n",
            "Character Error Rate (CER): 60.06%\n",
            "BLEU score: 29.74\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_31.txt\n",
            "Word Error Rate (WER): 20.53%\n",
            "Character Error Rate (CER): 15.09%\n",
            "BLEU score: 72.50\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_44.txt\n",
            "Word Error Rate (WER): 41.73%\n",
            "Character Error Rate (CER): 28.91%\n",
            "BLEU score: 55.69\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_25.txt\n",
            "Word Error Rate (WER): 21.97%\n",
            "Character Error Rate (CER): 14.48%\n",
            "BLEU score: 71.48\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_42.txt\n",
            "Word Error Rate (WER): 35.99%\n",
            "Character Error Rate (CER): 26.30%\n",
            "BLEU score: 51.74\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_24.txt\n",
            "Word Error Rate (WER): 56.24%\n",
            "Character Error Rate (CER): 40.07%\n",
            "BLEU score: 42.92\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_32.txt\n",
            "Word Error Rate (WER): 57.85%\n",
            "Character Error Rate (CER): 37.41%\n",
            "BLEU score: 43.92\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_21.txt\n",
            "Word Error Rate (WER): 76.49%\n",
            "Character Error Rate (CER): 53.81%\n",
            "BLEU score: 38.70\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_15.txt\n",
            "Word Error Rate (WER): 13.72%\n",
            "Character Error Rate (CER): 4.06%\n",
            "BLEU score: 75.89\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_38.txt\n",
            "Word Error Rate (WER): 27.68%\n",
            "Character Error Rate (CER): 20.12%\n",
            "BLEU score: 63.77\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_27.txt\n",
            "Word Error Rate (WER): 51.41%\n",
            "Character Error Rate (CER): 43.11%\n",
            "BLEU score: 41.27\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_17.txt\n",
            "Word Error Rate (WER): 35.41%\n",
            "Character Error Rate (CER): 27.14%\n",
            "BLEU score: 54.71\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_19.txt\n",
            "Word Error Rate (WER): 58.73%\n",
            "Character Error Rate (CER): 43.22%\n",
            "BLEU score: 38.64\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_18.txt\n",
            "Word Error Rate (WER): 18.98%\n",
            "Character Error Rate (CER): 12.17%\n",
            "BLEU score: 72.47\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_35.txt\n",
            "Word Error Rate (WER): 48.70%\n",
            "Character Error Rate (CER): 38.23%\n",
            "BLEU score: 46.98\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_40.txt\n",
            "Word Error Rate (WER): 41.94%\n",
            "Character Error Rate (CER): 26.11%\n",
            "BLEU score: 53.86\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_45.txt\n",
            "Word Error Rate (WER): 24.97%\n",
            "Character Error Rate (CER): 19.12%\n",
            "BLEU score: 66.49\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_11.txt\n",
            "Word Error Rate (WER): 28.57%\n",
            "Character Error Rate (CER): 20.58%\n",
            "BLEU score: 64.56\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_47.txt\n",
            "Word Error Rate (WER): 38.87%\n",
            "Character Error Rate (CER): 30.35%\n",
            "BLEU score: 55.39\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_12.txt\n",
            "Word Error Rate (WER): 68.16%\n",
            "Character Error Rate (CER): 40.83%\n",
            "BLEU score: 38.52\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_14.txt\n",
            "Word Error Rate (WER): 43.99%\n",
            "Character Error Rate (CER): 31.76%\n",
            "BLEU score: 52.38\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_8.txt\n",
            "Word Error Rate (WER): 61.76%\n",
            "Character Error Rate (CER): 40.99%\n",
            "BLEU score: 39.77\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_22.txt\n",
            "Word Error Rate (WER): 40.98%\n",
            "Character Error Rate (CER): 23.14%\n",
            "BLEU score: 51.96\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_7.txt\n",
            "Word Error Rate (WER): 67.13%\n",
            "Character Error Rate (CER): 40.40%\n",
            "BLEU score: 38.98\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_48.txt\n",
            "Word Error Rate (WER): 28.24%\n",
            "Character Error Rate (CER): 19.84%\n",
            "BLEU score: 63.05\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_39.txt\n",
            "Word Error Rate (WER): 53.23%\n",
            "Character Error Rate (CER): 45.91%\n",
            "BLEU score: 50.33\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_51.txt\n",
            "Word Error Rate (WER): 24.57%\n",
            "Character Error Rate (CER): 16.61%\n",
            "BLEU score: 66.22\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_5.txt\n",
            "Word Error Rate (WER): 39.67%\n",
            "Character Error Rate (CER): 24.43%\n",
            "BLEU score: 53.38\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_26.txt\n",
            "Word Error Rate (WER): 37.67%\n",
            "Character Error Rate (CER): 21.56%\n",
            "BLEU score: 54.30\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_6.txt\n",
            "Word Error Rate (WER): 64.65%\n",
            "Character Error Rate (CER): 45.92%\n",
            "BLEU score: 39.58\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_23.txt\n",
            "Word Error Rate (WER): 68.96%\n",
            "Character Error Rate (CER): 50.24%\n",
            "BLEU score: 38.38\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_20.txt\n",
            "Word Error Rate (WER): 16.13%\n",
            "Character Error Rate (CER): 9.95%\n",
            "BLEU score: 73.43\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_28.txt\n",
            "Word Error Rate (WER): 51.26%\n",
            "Character Error Rate (CER): 35.75%\n",
            "BLEU score: 41.42\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_29.txt\n",
            "Word Error Rate (WER): 58.70%\n",
            "Character Error Rate (CER): 36.50%\n",
            "BLEU score: 42.08\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_43.txt\n",
            "Word Error Rate (WER): 41.18%\n",
            "Character Error Rate (CER): 29.88%\n",
            "BLEU score: 50.68\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_49.txt\n",
            "Word Error Rate (WER): 60.83%\n",
            "Character Error Rate (CER): 41.76%\n",
            "BLEU score: 45.38\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_13.txt\n",
            "Word Error Rate (WER): 33.38%\n",
            "Character Error Rate (CER): 21.65%\n",
            "BLEU score: 59.49\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_3.txt\n",
            "Word Error Rate (WER): 26.56%\n",
            "Character Error Rate (CER): 13.97%\n",
            "BLEU score: 63.47\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_4.txt\n",
            "Word Error Rate (WER): 47.74%\n",
            "Character Error Rate (CER): 34.62%\n",
            "BLEU score: 49.82\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_2.txt\n",
            "Word Error Rate (WER): 15.08%\n",
            "Character Error Rate (CER): 8.83%\n",
            "BLEU score: 77.61\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_41.txt\n",
            "Word Error Rate (WER): 29.31%\n",
            "Character Error Rate (CER): 15.92%\n",
            "BLEU score: 60.27\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_16.txt\n",
            "Word Error Rate (WER): 71.19%\n",
            "Character Error Rate (CER): 44.09%\n",
            "BLEU score: 38.02\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_34.txt\n",
            "Word Error Rate (WER): 21.11%\n",
            "Character Error Rate (CER): 13.00%\n",
            "BLEU score: 69.28\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_46.txt\n",
            "Word Error Rate (WER): 29.88%\n",
            "Character Error Rate (CER): 20.76%\n",
            "BLEU score: 59.92\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_36.txt\n",
            "Word Error Rate (WER): 21.34%\n",
            "Character Error Rate (CER): 11.30%\n",
            "BLEU score: 67.71\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_30.txt\n",
            "Word Error Rate (WER): 59.45%\n",
            "Character Error Rate (CER): 40.18%\n",
            "BLEU score: 40.31\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_50.txt\n",
            "Word Error Rate (WER): 37.70%\n",
            "Character Error Rate (CER): 23.97%\n",
            "BLEU score: 49.55\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_10.txt\n",
            "Word Error Rate (WER): 47.80%\n",
            "Character Error Rate (CER): 28.86%\n",
            "BLEU score: 51.18\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_1.txt\n",
            "Word Error Rate (WER): 54.29%\n",
            "Character Error Rate (CER): 40.53%\n",
            "BLEU score: 39.68\n",
            "--------------------------------------------------\n",
            "Evaluating file: Case_33.txt\n",
            "Word Error Rate (WER): 77.80%\n",
            "Character Error Rate (CER): 42.35%\n",
            "BLEU score: 30.71\n",
            "--------------------------------------------------\n",
            "\n",
            "Average Metrics Across All Files:\n",
            "Average Word Error Rate (WER): 43.93%\n",
            "Average Character Error Rate (CER): 29.49%\n",
            "Average BLEU score: 52.60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas openpyxl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZueGJwBFyau",
        "outputId": "a751c37f-b92b-4727-acef-480fbd576f3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code that saves all the reults in a excel file"
      ],
      "metadata": {
        "id": "YCIqSfVJDoL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import editdistance\n",
        "import sacrebleu\n",
        "import string\n",
        "import pandas as pd\n",
        "\n",
        "# Function to compute Word Error Rate (WER)\n",
        "def compute_wer(reference, hypothesis):\n",
        "    reference_words = reference.split()\n",
        "    hypothesis_words = hypothesis.split()\n",
        "    return editdistance.eval(reference_words, hypothesis_words) / len(reference_words)\n",
        "\n",
        "# Function to compute Character Error Rate (CER)\n",
        "def compute_cer(reference, hypothesis):\n",
        "    return editdistance.eval(reference, hypothesis) / len(reference)\n",
        "\n",
        "# Function to compute BLEU score\n",
        "def compute_bleu(reference, hypothesis):\n",
        "    bleu = sacrebleu.corpus_bleu([hypothesis], [[reference]])\n",
        "    return bleu.score\n",
        "\n",
        "# Function to preprocess the text (remove punctuation and convert to lowercase)\n",
        "def preprocess_text(text):\n",
        "    # Convert text to lowercase and remove punctuation\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
        "    return text\n",
        "\n",
        "# Read the original text and transcribed text\n",
        "def read_text_files(original_file, transcribed_file):\n",
        "    with open(original_file, 'r') as f:\n",
        "        original_text = f.read().strip()\n",
        "\n",
        "    with open(transcribed_file, 'r') as f:\n",
        "        transcribed_text = f.read().strip()\n",
        "\n",
        "    return original_text, transcribed_text\n",
        "\n",
        "# Function to evaluate metrics for a single file\n",
        "def evaluate_single_file(reference, hypothesis):\n",
        "    # Preprocess the reference and hypothesis text\n",
        "    reference = preprocess_text(reference)\n",
        "    hypothesis = preprocess_text(hypothesis)\n",
        "\n",
        "    # Compute WER\n",
        "    wer = compute_wer(reference, hypothesis)\n",
        "\n",
        "    # Compute CER\n",
        "    cer = compute_cer(reference, hypothesis)\n",
        "\n",
        "    # Compute BLEU score\n",
        "    bleu_score = compute_bleu(reference, hypothesis)\n",
        "\n",
        "    return wer, cer, bleu_score\n",
        "\n",
        "# Function to evaluate metrics for a single model and store results\n",
        "def evaluate_metrics(input_folder, output_folder, model_name, evaluation_data):\n",
        "    # Get all text files in the input folder\n",
        "    input_files = [f for f in os.listdir(input_folder) if f.endswith('.txt')]\n",
        "\n",
        "    # Loop through each file in the input folder\n",
        "    for input_file in input_files:\n",
        "        # Create the corresponding output file path\n",
        "        input_file_path = os.path.join(input_folder, input_file)\n",
        "        output_file_path = os.path.join(output_folder, input_file)\n",
        "\n",
        "        # Check if the corresponding file exists in the output folder\n",
        "        if os.path.exists(output_file_path):\n",
        "            # If the file doesn't exist in evaluation_data, initialize it\n",
        "            if input_file not in evaluation_data:\n",
        "                evaluation_data[input_file] = {}\n",
        "\n",
        "            # Read original and transcribed text\n",
        "            reference, hypothesis = read_text_files(input_file_path, output_file_path)\n",
        "\n",
        "            # Evaluate and accumulate metrics for the current file\n",
        "            wer, cer, bleu_score = evaluate_single_file(reference, hypothesis)\n",
        "\n",
        "            # Store the result in the evaluation data under the model's columns\n",
        "            evaluation_data[input_file][model_name] = {\n",
        "                'WER': wer * 100,\n",
        "                'CER': cer * 100,\n",
        "                'BLEU': bleu_score\n",
        "            }\n",
        "        else:\n",
        "            print(f\"Warning: The file '{input_file}' is missing in the output folder.\")\n",
        "\n",
        "# Main function to evaluate all models and save results to a single file\n",
        "def evaluate_all_models(input_folder, whisper_output_folder, vosk_output_folder, wav2vec_output_folder):\n",
        "    # Initialize an empty dictionary to store evaluation results for all models\n",
        "    evaluation_data = {}\n",
        "\n",
        "    # Evaluate results for each model and append to the evaluation_data dictionary\n",
        "    evaluate_metrics(input_folder, whisper_output_folder, \"Whisper\", evaluation_data)\n",
        "    evaluate_metrics(input_folder, vosk_output_folder, \"Vosk\", evaluation_data)\n",
        "    evaluate_metrics(input_folder, wav2vec_output_folder, \"Wav2Vec2.0\", evaluation_data)\n",
        "\n",
        "    # If we have data to save\n",
        "    if evaluation_data:\n",
        "        # Prepare data in the format suitable for creating the DataFrame\n",
        "        data = []\n",
        "        for file_name, metrics in evaluation_data.items():\n",
        "            row = {'File Name': file_name}\n",
        "            for model, model_metrics in metrics.items():\n",
        "                row[f'{model} WER'] = model_metrics['WER']\n",
        "                row[f'{model} CER'] = model_metrics['CER']\n",
        "                row[f'{model} BLEU'] = model_metrics['BLEU']\n",
        "            data.append(row)\n",
        "\n",
        "        # Create a DataFrame from the evaluation data\n",
        "        df = pd.DataFrame(data)\n",
        "\n",
        "        # Save the DataFrame to an Excel file\n",
        "        output_excel_file = '/content/comparison_results.xlsx'\n",
        "        df.to_excel(output_excel_file, index=False, engine='openpyxl')\n",
        "\n",
        "        print(f\"Results saved to {output_excel_file}\")\n",
        "    else:\n",
        "        print(\"No files were processed.\")\n",
        "\n",
        "# Example usage:\n",
        "# Replace '/path/to/extracted_text' and '/path/to/transcribed_text' with actual folder paths\n",
        "input_folder = '/content/drive/MyDrive/Dataset/Case_Files/PT'  # Folder with original extracted texts\n",
        "whisper_output_folder = '/content/drive/MyDrive/Dataset/Case_Files/whisper_text_25Files'  # Folder with Whisper transcribed texts\n",
        "vosk_output_folder = '/content/drive/MyDrive/Dataset/Case_Files/vosk_text_25Files'  # Folder with Vosk transcribed texts\n",
        "wav2vec_output_folder = '/content/drive/MyDrive/Dataset/Case_Files/wav2vac2.0_text_25Files'  # Folder with wav2vec transcribed texts\n",
        "\n",
        "# Evaluate all models and save the results to a single file\n",
        "evaluate_all_models(input_folder, whisper_output_folder, vosk_output_folder, wav2vec_output_folder)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bD98VWDxGTwY",
        "outputId": "cb141da2-5357-4d34-9040-03e9cfd3c240"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to /content/comparison_results.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"/content/comparison_results.xlsx\"  # Update with your actual file path\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Display basic statistics\n",
        "print(df.describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YS9c3cakHuuX",
        "outputId": "24231135-fa14-4642-92e3-d9ff155c7ee8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              WER         CER  BLEU Score\n",
            "count  153.000000  153.000000  153.000000\n",
            "mean    41.020133   22.981340   52.406504\n",
            "std     25.108442   16.249775   22.530530\n",
            "min      3.080082    0.845277   10.557728\n",
            "25%     20.532319    7.584329   32.325785\n",
            "50%     37.700535   21.417017   52.380023\n",
            "75%     60.294118   37.411095   70.670207\n",
            "max     99.478488   61.579347   94.140398\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the best performing case based on BLEU Score\n",
        "best_case = df.loc[df[\"BLEU Score\"].idxmax()]\n",
        "print(\"Best Performing Case:\")\n",
        "print(best_case)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVLOAFyJTLGO",
        "outputId": "caa7dba4-94d9-421e-af14-481ecdc4c003"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Performing Case:\n",
            "File Name     Case_25.txt\n",
            "Model             Whisper\n",
            "WER              3.080082\n",
            "CER              0.845277\n",
            "BLEU Score      94.140398\n",
            "Name: 4, dtype: object\n"
          ]
        }
      ]
    }
  ]
}